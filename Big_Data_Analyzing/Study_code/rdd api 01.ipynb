{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ[\"PYLIB\"]=os.path.join(os.environ[\"SPARK_HOME\"],'python','lib')\n",
    "sys.path.insert(0,os.path.join(os.environ[\"PYLIB\"],'py4j-0.10.1-src.zip'))\n",
    "sys.path.insert(0,os.path.join(os.environ[\"PYLIB\"],'pyspark.zip'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "myConf=pyspark.SparkConf()\n",
    "spark = pyspark.sql.SparkSession.builder\\\n",
    "    .master(\"local\")\\\n",
    "    .appName(\"myApp\")\\\n",
    "    .config(conf=myConf)\\\n",
    "    .getOrCreate() #getOrCreate -> 세션이 이미 있으면 불러오고 아니면 생성한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "myRdd2 = spark.sparkContext\\\n",
    "    .textFile(os.path.join(\"data\", \"ds_spark_wiki.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'Wikipedia',\n",
       " u'Apache Spark is an open source cluster computing framework.',\n",
       " u'\\uc544\\ud30c\\uce58 \\uc2a4\\ud30c\\ud06c\\ub294 \\uc624\\ud508 \\uc18c\\uc2a4 \\ud074\\ub7ec\\uc2a4\\ud130 \\ucef4\\ud4e8\\ud305 \\ud504\\ub808\\uc784\\uc6cc\\ud06c\\uc774\\ub2e4.',\n",
       " u'Apache Spark Apache Spark Apache Spark Apache Spark',\n",
       " u'\\uc544\\ud30c\\uce58 \\uc2a4\\ud30c\\ud06c \\uc544\\ud30c\\uce58 \\uc2a4\\ud30c\\ud06c \\uc544\\ud30c\\uce58 \\uc2a4\\ud30c\\ud06c \\uc544\\ud30c\\uce58 \\uc2a4\\ud30c\\ud06c',\n",
       " u\"Originally developed at the University of California, Berkeley's AMPLab,\",\n",
       " u'the Spark codebase was later donated to the Apache Software Foundation,',\n",
       " u'which has maintained it since.',\n",
       " u'Spark provides an interface for programming entire clusters with',\n",
       " u'implicit data parallelism and fault-tolerance.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRdd2.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 아파 아파치 스파크는 오픈 소스 클러스터 컴퓨팅 프레임워크이다.\n",
      "아파 아파치 스파크 아파치 스파크 아파치 스파크 아파치 스파크\n",
      "-----\n",
      "im implicit data parallelism and fault-tolerance.\n",
      "-----\n",
      "th the Spark codebase was later donated to the Apache Software Foundation,\n",
      "-----\n",
      "Wi Wikipedia\n",
      "-----\n",
      "Ap Apache Spark is an open source cluster computing framework.\n",
      "Ap Apache Spark Apache Spark Apache Spark Apache Spark\n",
      "-----\n",
      "Sp Spark provides an interface for programming entire clusters with\n",
      "-----\n",
      "Or Originally developed at the University of California, Berkeley's AMPLab,\n",
      "-----\n",
      "wh which has maintained it since.\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "myRdd_group = myRdd2.groupBy(lambda x:x[0:2])\n",
    "# 위의 groupBy함수를 이용하면 () 안에 해당되는 인자가 key가 되어 이를 바탕으로 분류해준다.\n",
    "\n",
    "for (k, v) in myRdd_group.collect(): #groupBy를 통하여 나온 결과는 (key, value)로\n",
    "    for eachValue in v: #v는 그룹된 인자(문장단위이다.)\n",
    "        print k, eachValue #eachValue는 하나의 문장 \n",
    "    print \"-----\"  #출력 결과는 key eachValue가 v안에 있는 만큼 계속 나온 뒤 ----으로 분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "_testList=[(\"key1\",1),(\"key1\",1),(\"key1\",1),(\"key2\",1),(\"key2\",1),\n",
    "           (\"key1\",1),(\"key2\",1),\n",
    "           (\"key1\",1),(\"key1\",1),(\"key2\",1),(\"key2\",1)] \n",
    "#key1, key2로 명확히 key값이 분류된 정형 데이터로 이해해보자.\n",
    "_testRdd=spark.sparkContext.parallelize(_testList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('key2', <pyspark.resultiterable.ResultIterable at 0x6f66908>),\n",
       " ('key1', <pyspark.resultiterable.ResultIterable at 0x6f66940>)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_testRdd.groupBy(lambda x:x[0]).collect() \n",
    "#x[0]을 키값으로 사용\n",
    "#이 또한 출력결과가 Rdd이므로 Iterable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('key2', [('key2', 1), ('key2', 1), ('key2', 1), ('key2', 1), ('key2', 1)]),\n",
       " ('key1',\n",
       "  [('key1', 1),\n",
       "   ('key1', 1),\n",
       "   ('key1', 1),\n",
       "   ('key1', 1),\n",
       "   ('key1', 1),\n",
       "   ('key1', 1)])]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_testRdd.groupBy(lambda x:x[0]).mapValues(lambda x:list(x)).collect()\n",
    "#mapvalues(lambda x:list(x))를 통해 groupBy Rdd 고유의 type을 list type으로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
