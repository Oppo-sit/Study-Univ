{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark Transfrom(ETL)\n",
    "\n",
    "데이터의 구성\n",
    "    \n",
    "    데이터 컬럼 명은 'lableCol', 'featureCol'으로 설정해서 사용할 수 있다.\n",
    "    \n",
    "    기본 컬럼 명을 사용하는 경우:\n",
    "        구분\t컬럼 구성\n",
    "        DataFrame\t'label' (DoubleType), 'features' (sparse or dense vectors)\n",
    "        RDD\tLabeledPoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataFrame 변환"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    RDD는 map-reduce를 사용하지만 DataFrame은 사용 하지 않는다.\n",
    "        대신 Transformer와 Estimator를 사용하는데, 여러 Estimator를 \n",
    "        Pipeline을 적용하여 값을 반환한다.\n",
    "        \n",
    "    Estimator : 모델의 인자 설정, 이를 데이터에 적용\n",
    "        Transformer를 반환, fit() 함수를 사용\n",
    "    Transformer : 열을 선택, 변환. \n",
    "        DataFrame으로 반환, transform() 함수를 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "특징 추출 -> Feature vectors\n",
    "분류 -> Class or Label(같은 의미)\n",
    "\n",
    "텍스트는 'Bag of words'로 표현\n",
    "    =>단어의 순서는 의미가 없다!\n",
    "    \n",
    "    corpus 문서 집합 \n",
    "    document 레코드 (문장을 문서로 취급할 수 있다.)\n",
    "    vocabulary 중복없는 단어 집합\n",
    "    word vector 있다-없다, 단어빈도, 등등..\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ[\"PYLIB\"]=os.path.join(os.environ[\"SPARK_HOME\"],'python','lib')\n",
    "sys.path.insert(0,os.path.join(os.environ[\"PYLIB\"],'py4j-0.10.1-src.zip'))\n",
    "sys.path.insert(0,os.path.join(os.environ[\"PYLIB\"],'pyspark.zip'))\n",
    "\n",
    "import pyspark\n",
    "myConf=pyspark.SparkConf()\n",
    "spark = pyspark.sql.SparkSession.builder\\\n",
    "    .master(\"local\")\\\n",
    "    .appName(\"myApp\")\\\n",
    "    .config(\"spark.sql.warehouse.dir\", \"C:/Users/G312\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc=[\n",
    "    \"When I find myself in times of trouble\",\n",
    "    \"Mother Mary comes to me\",\n",
    "    \"Speaking words of wisdom, let it be\",\n",
    "    \"And in my hour of darkness\",\n",
    "    \"She is standing right in front of me\",\n",
    "    \"Speaking words of wisdom, let it be\",\n",
    "    \"Let it be\",\n",
    "    \"Let it be\",\n",
    "    \"Let it be\",\n",
    "    \"Let it be\",\n",
    "    \"Whisper words of wisdom, let it be\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "right 1\n",
      "be 7\n",
      "is 1\n",
      "When 1\n",
      "it 7\n",
      "in 3\n",
      "Mary 1\n",
      "Speaking 2\n",
      "standing 1\n",
      "darkness 1\n",
      "find 1\n",
      "wisdom, 3\n",
      "to 1\n",
      "Let 4\n",
      "And 1\n",
      "I 1\n",
      "let 3\n",
      "She 1\n",
      "words 3\n",
      "Mother 1\n",
      "front 1\n",
      "trouble 1\n",
      "me 2\n",
      "myself 1\n",
      "hour 1\n",
      "of 6\n",
      "times 1\n",
      "Whisper 1\n",
      "my 1\n",
      "comes 1\n"
     ]
    }
   ],
   "source": [
    "#Python으로 단어 빈도를 계산한다.\n",
    "d={}\n",
    "for sentence in doc:\n",
    "    words=sentence.split()\n",
    "    for word in words:\n",
    "        if word in d:\n",
    "            d[word]+=1\n",
    "        else:\n",
    "            d[word]=1\n",
    "            \n",
    "for k,v in d.iteritems():\n",
    "    print k,v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TfidfTransformer는 TF-IDF(Term Frequency-Inverse Document Frequency)를 계산한다.\n",
    "    \n",
    "    단계 1: Tokenizer를 사용하여 문장을 단어로 분리\n",
    "    단계 2: CountVectorizer를 사용하여 단어의 빈도수tf를 계산\n",
    "    단계 3: HashingTF를 사용하여 'word vector'를 계산. HashingTF은 hash함수에 따라 \n",
    "    단어의 고유번호를 생성, hash고유번호의 충돌 가능성을 줄이기 위해, \n",
    "    단어 수를 제한할 수 있다.\n",
    "    단계 4: IDF를 계산\n",
    "    단계 5: TF-IDF를 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.09861228867\n"
     ]
    }
   ],
   "source": [
    "#TF-IDF는 머신러닝에서 단어의 '중요성'을 나타내기 위해 사용하는 계산법.\n",
    "\n",
    "#TfidfTransformer는 TF-IDF를 계산.\n",
    "#TF(Term Frequency) 계산\n",
    "    #stopwords를 제외한 한 단어의 노출 빈도.\n",
    "#df(Document frequency) 계산\n",
    "    #한 단어가 문서(문장)에 나타난 빈도.\n",
    "#N(number of documents) 계산\n",
    "    #전체 문서의 수\n",
    "#idf(inverse document frequency) 계산\n",
    "    #단어가 나타난 문서의 비율을 거꾸로 계산.\n",
    "    #0으로 나뉘는 것을 방지하기 위해 +1을 해준다.\n",
    "    \n",
    "#위의 문서에서 'wisdom'이란 단어의 TF-IDF를 계산해본다. \n",
    "import math\n",
    "tf=1./4 #단어 4개가 빈도 1이므로 wisdom 단어의 빈도는 1./4\n",
    "df=3.\n",
    "N=11.\n",
    "idf=math.log((N+1)/(df+1))+1 #log는 자연로그.\n",
    "print idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark는 'sklearn'의 TF-IDF와 동일한 방식으로 계산한다. CountVectorizer를 사용하여, 문서 x 단어를 표로 계산결과를 출력할 수 있다. 그 다음으로, TF-IDF를 계산할 수 있다. 이 때 (문서id, 단어id) 별로 결과가 출력된다.\n",
    "\n",
    "아래는 sklearn을 이용한 방식."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#countVectorizer로 문서 X 단어를 표로 하여 계산결과를 출력한다.\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer = \"word\",   \\\n",
    "                             tokenizer = None,    \\\n",
    "                             preprocessor = None, \\\n",
    "                             stop_words = None,   \\\n",
    "                             max_features = 5000) \n",
    "vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 1 1 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 1]\n",
      " [1 0 0 1 0 0 1 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 0 1 1 0 0 0 1 0 0 0 1 1 1 0 1 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 1]\n",
      " [0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "print vectorizer.fit_transform(doc).todense() #Word Vector를 출력한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'and': 0, u'be': 1, u'right': 17, u'whisper': 25, u'is': 8, u'it': 9, u'wisdom': 26, u'me': 12, u'let': 10, u'words': 27, u'in': 7, u'front': 5, u'trouble': 23, u'find': 4, u'standing': 20, u'comes': 2, u'myself': 15, u'darkness': 3, u'hour': 6, u'of': 16, u'when': 24, u'times': 21, u'to': 22, u'she': 18, u'mother': 13, u'my': 14, u'mary': 11, u'speaking': 19}\n"
     ]
    }
   ],
   "source": [
    "print vectorizer.vocabulary_ #단어의 일련번호(자동 부여된 상태)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TfidfVectorizer를 사용해서 계산\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(max_df=1.0, min_df=1, stop_words='english',norm = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 9)\t2.791759469228055\n",
      "  (0, 10)\t2.791759469228055\n",
      "  (1, 5)\t2.791759469228055\n",
      "  (1, 4)\t2.791759469228055\n",
      "  (1, 0)\t2.791759469228055\n",
      "  (2, 7)\t2.386294361119891\n",
      "  (2, 13)\t2.09861228866811\n",
      "  (2, 12)\t2.09861228866811\n",
      "  (2, 3)\t1.4054651081081644\n",
      "  (3, 2)\t2.791759469228055\n",
      "  (3, 1)\t2.791759469228055\n",
      "  (4, 8)\t2.791759469228055\n",
      "  (4, 6)\t2.791759469228055\n",
      "  (5, 7)\t2.386294361119891\n",
      "  (5, 13)\t2.09861228866811\n",
      "  (5, 12)\t2.09861228866811\n",
      "  (5, 3)\t1.4054651081081644\n",
      "  (6, 3)\t1.4054651081081644\n",
      "  (7, 3)\t1.4054651081081644\n",
      "  (8, 3)\t1.4054651081081644\n",
      "  (9, 3)\t1.4054651081081644\n",
      "  (10, 13)\t2.09861228866811\n",
      "  (10, 12)\t2.09861228866811\n",
      "  (10, 3)\t1.4054651081081644\n",
      "  (10, 11)\t2.791759469228055\n"
     ]
    }
   ],
   "source": [
    "print vectorizer.fit_transform(doc) \n",
    "#출력값 : (문서번호, 단어번호)       TF-IDF value\n",
    "#(2, 12) -> 3번째 문서번호의 'wisdom'에 대한 TF-IDF를 알 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'standing': 8, u'right': 6, u'darkness': 1, u'hour': 2, u'whisper': 11, u'times': 9, u'let': 3, u'speaking': 7, u'words': 13, u'mother': 5, u'trouble': 10, u'wisdom': 12, u'mary': 4, u'comes': 0}\n"
     ]
    }
   ],
   "source": [
    "print vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.79175947 2.79175947 2.79175947 1.40546511 2.79175947 2.79175947\n",
      " 2.79175947 2.38629436 2.79175947 2.79175947 2.79175947 2.79175947\n",
      " 2.09861229 2.09861229]\n"
     ]
    }
   ],
   "source": [
    "print vectorizer.idf_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment : 긍정, 부정이 존재\n",
    "Sentiment analysis : 감성 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------+\n",
      "|sent                                  |\n",
      "+--------------------------------------+\n",
      "|When I find myself in times of trouble|\n",
      "|Mother Mary comes to me               |\n",
      "|Speaking words of wisdom, let it be   |\n",
      "|And in my hour of darkness            |\n",
      "|She is standing right in front of me  |\n",
      "|Speaking words of wisdom, let it be   |\n",
      "|우리 Let it be                          |\n",
      "|나 Let it be                           |\n",
      "|너 Let it be                           |\n",
      "|Let it be                             |\n",
      "|Whisper words of wisdom, let it be    |\n",
      "+--------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "doc=[\n",
    "    [\"When I find myself in times of trouble\"],\n",
    "    [\"Mother Mary comes to me\"],\n",
    "    [\"Speaking words of wisdom, let it be\"],\n",
    "    [\"And in my hour of darkness\"],\n",
    "    [\"She is standing right in front of me\"],\n",
    "    [\"Speaking words of wisdom, let it be\"],\n",
    "    [u\"우리 Let it be\"],\n",
    "    [u\"나 Let it be\"],\n",
    "    [u\"너 Let it be\"],\n",
    "    [\"Let it be\"],\n",
    "    [\"Whisper words of wisdom, let it be\"]\n",
    "]\n",
    "\n",
    "myDf=spark.createDataFrame(doc,['sent'])\n",
    "myDf.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# StringIndexer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "변수의 성격\n",
    "\n",
    "        구분        설명\t                           예\n",
    "\n",
    "        nominal     명목 또는 구분 값 cateogory        사자, 호랑이, 사람\n",
    "\n",
    "        ordinal     명목값과 다른 점은 순서가 있다.    키 low, med, high\n",
    "\n",
    "        interval    일정한 간격이 있다.                150-165, 165-180, 180-195\n",
    "\n",
    "문자를 인덱스 값(double)로 저장한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+\n",
      "|                sent|sentLabel|\n",
      "+--------------------+---------+\n",
      "|When I find mysel...|      9.0|\n",
      "|Mother Mary comes...|      8.0|\n",
      "|Speaking words of...|      0.0|\n",
      "|And in my hour of...|      5.0|\n",
      "|She is standing r...|      4.0|\n",
      "|Speaking words of...|      0.0|\n",
      "|        우리 Let it be|      6.0|\n",
      "|         나 Let it be|      1.0|\n",
      "|         너 Let it be|      2.0|\n",
      "|           Let it be|      7.0|\n",
      "|Whisper words of ...|      3.0|\n",
      "+--------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "labelIndexer = StringIndexer(inputCol=\"sent\", outputCol=\"sentLabel\")\n",
    "model=labelIndexer.fit(myDf)\n",
    "siDf=model.transform(myDf)\n",
    "siDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "문장을 단어와 같은 token으로 분리한다.\n",
    "\n",
    "단어는 배열로 구성한다. 요소는 string이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(sent=u'When I find myself in times of trouble', words=[u'when', u'i', u'find', u'myself', u'in', u'times', u'of', u'trouble'])\n",
      "Row(sent=u'Mother Mary comes to me', words=[u'mother', u'mary', u'comes', u'to', u'me'])\n",
      "Row(sent=u'Speaking words of wisdom, let it be', words=[u'speaking', u'words', u'of', u'wisdom,', u'let', u'it', u'be'])\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Tokenizer\n",
    "tokenizer = Tokenizer(inputCol=\"sent\", outputCol=\"words\")\n",
    "tokDf = tokenizer.transform(myDf)\n",
    "for r in tokDf.select(\"sent\", \"words\").take(3):\n",
    "    print r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RegTokenizer\n",
    "\n",
    "Regular Expression Tokenizer\n",
    "\n",
    "단어를 분리하기 위한 패턴을 적용할 수 있다.\n",
    "\n",
    "한글에는 \\w 패턴이 적용되지 않는다.\n",
    "\n",
    "whitespace \\s 패턴을 적용한다. 공백, TAB, CR, New Line 등이 해당된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|                sent|            wordsReg|\n",
      "+--------------------+--------------------+\n",
      "|When I find mysel...|[when, i, find, m...|\n",
      "|Mother Mary comes...|[mother, mary, co...|\n",
      "|Speaking words of...|[speaking, words,...|\n",
      "|And in my hour of...|[and, in, my, hou...|\n",
      "|She is standing r...|[she, is, standin...|\n",
      "|Speaking words of...|[speaking, words,...|\n",
      "|        우리 Let it be|   [우리, let, it, be]|\n",
      "|         나 Let it be|    [나, let, it, be]|\n",
      "|         너 Let it be|    [너, let, it, be]|\n",
      "|           Let it be|       [let, it, be]|\n",
      "|Whisper words of ...|[whisper, words, ...|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import RegexTokenizer\n",
    "re = RegexTokenizer(inputCol=\"sent\", outputCol=\"wordsReg\", pattern=\"\\\\s+\")\n",
    "reDf=re.transform(myDf)\n",
    "reDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stopwords\n",
    "한 단어 등 불용어.\n",
    "\n",
    "Tokenize를 거친 후 사용해야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover\n",
    "stop = StopWordsRemover(inputCol=\"wordsReg\", outputCol=\"nostops\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StopWordsRemover_4ce7a7977fb99c486253"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords=list()\n",
    "_stopwords=stop.getStopWords()\n",
    "for e in _stopwords:\n",
    "    stopwords.append(e)\n",
    "\n",
    "_mystopwords=[u\"나\",u\"너\", u\"우리\"]\n",
    "for e in _mystopwords:\n",
    "    stopwords.append(e)\n",
    "stop.setStopWords(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i me my myself we our ours ourselves you your yours yourself yourselves he him his himself she her hers herself it its itself they them their theirs themselves what which who whom this that these those am is are was were be been being have has had having do does did doing a an the and but if or because as until while of at by for with about against between into through during before after above below to from up down in out on off over under again further then once here there when where why how all any both each few more most other some such no nor not only own same so than too very s t can will just don should now d ll m o re ve y ain aren couldn didn doesn hadn hasn haven isn ma mightn mustn needn shan shouldn wasn weren won wouldn 나 너 우리\n"
     ]
    }
   ],
   "source": [
    "for e in stop.getStopWords():\n",
    "    print e,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|                sent|            wordsReg|             nostops|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|When I find mysel...|[when, i, find, m...|[find, times, tro...|\n",
      "|Mother Mary comes...|[mother, mary, co...|[mother, mary, co...|\n",
      "|Speaking words of...|[speaking, words,...|[speaking, words,...|\n",
      "|And in my hour of...|[and, in, my, hou...|    [hour, darkness]|\n",
      "|She is standing r...|[she, is, standin...|[standing, right,...|\n",
      "|Speaking words of...|[speaking, words,...|[speaking, words,...|\n",
      "|        우리 Let it be|   [우리, let, it, be]|               [let]|\n",
      "|         나 Let it be|    [나, let, it, be]|               [let]|\n",
      "|         너 Let it be|    [너, let, it, be]|               [let]|\n",
      "|           Let it be|       [let, it, be]|               [let]|\n",
      "|Whisper words of ...|[whisper, words, ...|[whisper, words, ...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stopDf=stop.transform(reDf)\n",
    "stopDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------+-----------------------------------------------+-------------------------------+\n",
      "|sent                                  |wordsReg                                       |nostops                        |\n",
      "+--------------------------------------+-----------------------------------------------+-------------------------------+\n",
      "|When I find myself in times of trouble|[when, i, find, myself, in, times, of, trouble]|[find, times, trouble]         |\n",
      "|Mother Mary comes to me               |[mother, mary, comes, to, me]                  |[mother, mary, comes]          |\n",
      "|Speaking words of wisdom, let it be   |[speaking, words, of, wisdom,, let, it, be]    |[speaking, words, wisdom,, let]|\n",
      "|And in my hour of darkness            |[and, in, my, hour, of, darkness]              |[hour, darkness]               |\n",
      "|She is standing right in front of me  |[she, is, standing, right, in, front, of, me]  |[standing, right, front]       |\n",
      "|Speaking words of wisdom, let it be   |[speaking, words, of, wisdom,, let, it, be]    |[speaking, words, wisdom,, let]|\n",
      "|우리 Let it be                          |[우리, let, it, be]                              |[let]                          |\n",
      "|나 Let it be                           |[나, let, it, be]                               |[let]                          |\n",
      "|너 Let it be                           |[너, let, it, be]                               |[let]                          |\n",
      "|Let it be                             |[let, it, be]                                  |[let]                          |\n",
      "|Whisper words of wisdom, let it be    |[whisper, words, of, wisdom,, let, it, be]     |[whisper, words, wisdom,, let] |\n",
      "+--------------------------------------+-----------------------------------------------+-------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stopDf.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CountVectorizer\n",
    "입력: a collection of text documents\n",
    "\n",
    "출력: word vector (sparse) vocabulary x TF\n",
    "\n",
    "tokenize하고 나서 사용(Stopwords 제거도 해야한다.)\n",
    "\n",
    "minDF\n",
    "\n",
    "    소수점은 비율, 사용된 문서 수/전체 문서 수\n",
    "        정수는 사용된 문서 수, 단어가 몇 개의 문서에 사용되어야 하는지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|                sent|             nostops|                  cv|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|When I find mysel...|[find, times, tro...|(16,[5,6,8],[1.0,...|\n",
      "|Mother Mary comes...|[mother, mary, co...|(16,[10,13,14],[1...|\n",
      "|Speaking words of...|[speaking, words,...|(16,[0,1,2,3],[1....|\n",
      "|And in my hour of...|    [hour, darkness]|(16,[7,9],[1.0,1.0])|\n",
      "|She is standing r...|[standing, right,...|(16,[4,12,15],[1....|\n",
      "|Speaking words of...|[speaking, words,...|(16,[0,1,2,3],[1....|\n",
      "|        우리 Let it be|               [let]|      (16,[0],[1.0])|\n",
      "|         나 Let it be|               [let]|      (16,[0],[1.0])|\n",
      "|         너 Let it be|               [let]|      (16,[0],[1.0])|\n",
      "|           Let it be|               [let]|      (16,[0],[1.0])|\n",
      "|Whisper words of ...|[whisper, words, ...|(16,[0,1,2,11],[1...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "\n",
      "let wisdom, words speaking right trouble find hour times darkness mother whisper front mary comes standing\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import CountVectorizer\n",
    "cv = CountVectorizer(inputCol=\"nostops\", outputCol=\"cv\",\n",
    "    vocabSize=30,minDF=1.0) #cv는 sparse vector로 표현된다.\n",
    "cvModel = cv.fit(stopDf)\n",
    "cvDf = cvModel.transform(stopDf)\n",
    "\n",
    "cvDf.collect()\n",
    "cvDf.select('sent','nostops','cv').show()\n",
    "for v in cvModel.vocabulary:\n",
    "    print v,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------+-------------------------------+---------------------------------+\n",
      "|sent                                  |nostops                        |cv                               |\n",
      "+--------------------------------------+-------------------------------+---------------------------------+\n",
      "|When I find myself in times of trouble|[find, times, trouble]         |(16,[5,6,8],[1.0,1.0,1.0])       |\n",
      "|Mother Mary comes to me               |[mother, mary, comes]          |(16,[10,13,14],[1.0,1.0,1.0])    |\n",
      "|Speaking words of wisdom, let it be   |[speaking, words, wisdom,, let]|(16,[0,1,2,3],[1.0,1.0,1.0,1.0]) |\n",
      "|And in my hour of darkness            |[hour, darkness]               |(16,[7,9],[1.0,1.0])             |\n",
      "|She is standing right in front of me  |[standing, right, front]       |(16,[4,12,15],[1.0,1.0,1.0])     |\n",
      "|Speaking words of wisdom, let it be   |[speaking, words, wisdom,, let]|(16,[0,1,2,3],[1.0,1.0,1.0,1.0]) |\n",
      "|우리 Let it be                          |[let]                          |(16,[0],[1.0])                   |\n",
      "|나 Let it be                           |[let]                          |(16,[0],[1.0])                   |\n",
      "|너 Let it be                           |[let]                          |(16,[0],[1.0])                   |\n",
      "|Let it be                             |[let]                          |(16,[0],[1.0])                   |\n",
      "|Whisper words of wisdom, let it be    |[whisper, words, wisdom,, let] |(16,[0,1,2,11],[1.0,1.0,1.0,1.0])|\n",
      "+--------------------------------------+-------------------------------+---------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cvDf.select('sent','nostops','cv').show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF\n",
    "Term frequency-inverse document frequency (TF-IDF)\n",
    "\n",
    "tokenizer하고 나서 사용해야 함.\n",
    "\n",
    "HashingTF : 고정길이 word vectors.\n",
    " \n",
    "IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(nostops=[u'find', u'times', u'trouble'], hash=SparseVector(50, {10: 1.0, 24: 1.0, 43: 1.0}))\n",
      "Row(nostops=[u'mother', u'mary', u'comes'], hash=SparseVector(50, {1: 1.0, 21: 1.0, 24: 1.0}))\n",
      "Row(nostops=[u'speaking', u'words', u'wisdom,', u'let'], hash=SparseVector(50, {9: 1.0, 12: 1.0, 14: 1.0, 41: 1.0}))\n",
      "Row(nostops=[u'hour', u'darkness'], hash=SparseVector(50, {23: 1.0, 27: 1.0}))\n",
      "Row(nostops=[u'standing', u'right', u'front'], hash=SparseVector(50, {24: 1.0, 43: 1.0, 46: 1.0}))\n",
      "Row(nostops=[u'speaking', u'words', u'wisdom,', u'let'], hash=SparseVector(50, {9: 1.0, 12: 1.0, 14: 1.0, 41: 1.0}))\n",
      "Row(nostops=[u'let'], hash=SparseVector(50, {14: 1.0}))\n",
      "Row(nostops=[u'let'], hash=SparseVector(50, {14: 1.0}))\n",
      "Row(nostops=[u'let'], hash=SparseVector(50, {14: 1.0}))\n",
      "Row(nostops=[u'let'], hash=SparseVector(50, {14: 1.0}))\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "\n",
    "hashTF = HashingTF(inputCol=\"nostops\", outputCol=\"hash\", numFeatures=50)\n",
    "hashDf = hashTF.transform(stopDf)\n",
    "idf = IDF(inputCol=\"hash\", outputCol=\"idf\")\n",
    "idfModel = idf.fit(hashDf)\n",
    "idfDf = idfModel.transform(hashDf)\n",
    "for e in idfDf.select(\"nostops\",\"hash\").take(10):\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NGram\n",
    "unigram은 한 단어로, bigram은 두 단어로 구성한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------+-----------------------------------------------+--------------------------------------------------------------------------+\n",
      "|sent                                  |words                                          |ngrams                                                                    |\n",
      "+--------------------------------------+-----------------------------------------------+--------------------------------------------------------------------------+\n",
      "|When I find myself in times of trouble|[when, i, find, myself, in, times, of, trouble]|[when i, i find, find myself, myself in, in times, times of, of trouble]  |\n",
      "|Mother Mary comes to me               |[mother, mary, comes, to, me]                  |[mother mary, mary comes, comes to, to me]                                |\n",
      "|Speaking words of wisdom, let it be   |[speaking, words, of, wisdom,, let, it, be]    |[speaking words, words of, of wisdom,, wisdom, let, let it, it be]        |\n",
      "|And in my hour of darkness            |[and, in, my, hour, of, darkness]              |[and in, in my, my hour, hour of, of darkness]                            |\n",
      "|She is standing right in front of me  |[she, is, standing, right, in, front, of, me]  |[she is, is standing, standing right, right in, in front, front of, of me]|\n",
      "|Speaking words of wisdom, let it be   |[speaking, words, of, wisdom,, let, it, be]    |[speaking words, words of, of wisdom,, wisdom, let, let it, it be]        |\n",
      "|우리 Let it be                          |[우리, let, it, be]                              |[우리 let, let it, it be]                                                   |\n",
      "|나 Let it be                           |[나, let, it, be]                               |[나 let, let it, it be]                                                    |\n",
      "|너 Let it be                           |[너, let, it, be]                               |[너 let, let it, it be]                                                    |\n",
      "|Let it be                             |[let, it, be]                                  |[let it, it be]                                                           |\n",
      "|Whisper words of wisdom, let it be    |[whisper, words, of, wisdom,, let, it, be]     |[whisper words, words of, of wisdom,, wisdom, let, let it, it be]         |\n",
      "+--------------------------------------+-----------------------------------------------+--------------------------------------------------------------------------+\n",
      "\n",
      "Row(words=[u'when', u'i', u'find', u'myself', u'in', u'times', u'of', u'trouble'], ngrams=[u'when i', u'i find', u'find myself', u'myself in', u'in times', u'times of', u'of trouble'])\n",
      "Row(words=[u'mother', u'mary', u'comes', u'to', u'me'], ngrams=[u'mother mary', u'mary comes', u'comes to', u'to me'])\n",
      "Row(words=[u'speaking', u'words', u'of', u'wisdom,', u'let', u'it', u'be'], ngrams=[u'speaking words', u'words of', u'of wisdom,', u'wisdom, let', u'let it', u'it be'])\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import NGram\n",
    "ngram = NGram(n=2, inputCol=\"words\", outputCol=\"ngrams\") #n=2 => bigram으로 구성되었다.\n",
    "ngramDf = ngram.transform(tokDf)\n",
    "ngramDf.show(truncate=False)\n",
    "for e in ngramDf.select(\"words\",\"ngrams\").take(3):\n",
    "    print e\n",
    "    \n",
    "#글에서 감정/의견을 나타내는 부분의 범주에 맞추어 n의 값을 설정한다.\n",
    "#ex) n=1 '좋다', '나쁘다' n=2 '영화가 좋다', '영화가 나쁘다'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 연속데이터의 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "rdd=spark.sparkContext\\\n",
    "    .textFile(os.path.join('data','ds_spark_heightweight.txt'))\n",
    "\n",
    "myRdd=rdd.map(lambda line:[float(x) for x in line.split('\\t')])\n",
    "myDf=spark.createDataFrame(myRdd,[\"id\",\"weight\",\"height\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+------+-------+\n",
      "|  id|weight|height|weight2|\n",
      "+----+------+------+-------+\n",
      "| 1.0| 65.78|112.99|    0.0|\n",
      "| 2.0| 71.52|136.49|    1.0|\n",
      "| 3.0|  69.4|153.03|    1.0|\n",
      "| 4.0| 68.22|142.34|    1.0|\n",
      "| 5.0| 67.79| 144.3|    0.0|\n",
      "| 6.0|  68.7| 123.3|    1.0|\n",
      "| 7.0|  69.8|141.49|    1.0|\n",
      "| 8.0| 70.01|136.46|    1.0|\n",
      "| 9.0|  67.9|112.37|    0.0|\n",
      "|10.0| 66.78|120.67|    0.0|\n",
      "+----+------+------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Binarizer\n",
    "binarizer = Binarizer(threshold=68.0, inputCol=\"weight\", outputCol=\"weight2\")\n",
    "binDf = binarizer.transform(myDf)\n",
    "binDf.show(10)\n",
    "#binerizer : threshold(임계값)에 따라 0/1의 값으로 데이터를 구분한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+------+-------+-------+\n",
      "|  id|weight|height|weight2|height3|\n",
      "+----+------+------+-------+-------+\n",
      "| 1.0| 65.78|112.99|    0.0|    0.0|\n",
      "| 2.0| 71.52|136.49|    1.0|    1.0|\n",
      "| 3.0|  69.4|153.03|    1.0|    2.0|\n",
      "| 4.0| 68.22|142.34|    1.0|    2.0|\n",
      "| 5.0| 67.79| 144.3|    0.0|    2.0|\n",
      "| 6.0|  68.7| 123.3|    1.0|    0.0|\n",
      "| 7.0|  69.8|141.49|    1.0|    2.0|\n",
      "| 8.0| 70.01|136.46|    1.0|    1.0|\n",
      "| 9.0|  67.9|112.37|    0.0|    0.0|\n",
      "|10.0| 66.78|120.67|    0.0|    0.0|\n",
      "+----+------+------+-------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import QuantileDiscretizer\n",
    "\n",
    "discretizer = QuantileDiscretizer(numBuckets=3, inputCol=\"height\", outputCol=\"height3\")\n",
    "qdDf = discretizer.fit(binDf).transform(binDf)\n",
    "qdDf.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VectorAssembler\n",
    "열을 묶어서 Vector열로 만든다.\n",
    "\n",
    "string은 묶을 수 없다.\n",
    "\n",
    "pyspark.ml.linalg.Vectors를 사용한다. (주의: pyspark.mllib.linalg.Vectors를 사용하지 않는다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: double (nullable = true)\n",
      " |-- weight: double (nullable = true)\n",
      " |-- height: double (nullable = true)\n",
      " |-- weight2: double (nullable = true)\n",
      " |-- height3: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n",
      "+---+------+------+-------+-------+---------+\n",
      "| id|weight|height|weight2|height3| features|\n",
      "+---+------+------+-------+-------+---------+\n",
      "|1.0| 65.78|112.99|    0.0|    0.0|(2,[],[])|\n",
      "|2.0| 71.52|136.49|    1.0|    1.0|[1.0,1.0]|\n",
      "|3.0|  69.4|153.03|    1.0|    2.0|[1.0,2.0]|\n",
      "|4.0| 68.22|142.34|    1.0|    2.0|[1.0,2.0]|\n",
      "|5.0| 67.79| 144.3|    0.0|    2.0|[0.0,2.0]|\n",
      "+---+------+------+-------+-------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#머신러닝에 필요한 column : label, features\n",
    "#vectorAssembler로 features를 생성한다.\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "va = VectorAssembler(inputCols=[\"weight2\",\"height3\"],outputCol=\"features\") \n",
    "vaDf = va.transform(qdDf)\n",
    "vaDf.printSchema()\n",
    "vaDf.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|sent                                                                                                                                 |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|존경하는 국민 여러분, 경찰관 여러분, 일흔네 돌 ‘경찰의 날’입니다.                                                                                              |\n",
      "|                                                                                                                                     |\n",
      "|국민의 안전을 위해 밤낮없이 애쓰시는 전국의 15만 경찰관 여러분께 먼저 감사를 드립니다. 전몰·순직 경찰관들의 고귀한 희생에 경의를 표합니다. 유가족 여러분께 위로의 마음을 전합니다.                              |\n",
      "|                                                                                                                                     |\n",
      "|오늘 홍조근정훈장을 받으신 중앙경찰학교장 이은정 치안감님, 근정포장을 받으신 광주남부경찰서 김동현 경감님을 비롯한 수상자 여러분께 각별한 축하와 감사를 드립니다. 또한 경찰 영웅으로 추서되신 차일혁, 최중락님께 국민의 사랑을 전해드립니다.|\n",
      "|                                                                                                                                     |\n",
      "|사랑하는 경찰관 여러분,                                                                                                                        |\n",
      "|                                                                                                                                     |\n",
      "|여러분의 헌신적 노력으로 우리의 치안은 참 좋아졌습니다. 지난해 범죄 발생은 2015년에 비해 15.1% 줄었습니다. 같은 기간 교통사고 사망자는 18.2% 감소했습니다.                                      |\n",
      "|                                                                                                                                     |\n",
      "|치안의 개선은 국민의 체감으로 나타나고 있습니다. 올해 상반기 국민의 체감안전도는 74.5점으로 역대 최고를 기록했습니다. 범죄안전도는 처음으로 80점을 넘었습니다.                                         |\n",
      "|                                                                                                                                     |\n",
      "|한국을 찾는 외국 관광객들도 우리의 치안을 가장 좋게 평가합니다. 한국의 무엇이 좋았느냐는 물음에 외국 관광객들은 7년 연속으로 치안이 가장 좋았다고 응답했습니다. 개발도상국들은 우리의 경찰을 모범으로 삼으려 합니다.            |\n",
      "|                                                                                                                                     |\n",
      "|올해는 ‘경찰의 날’에 맞춰 국제치안산업박람회와 서울국제경찰청장회의가 함께 열립니다. 우리의 치안 발전과 치안산업 발전이 세계에 더 널리 알려지게 될 것입니다.                                            |\n",
      "|                                                                                                                                     |\n",
      "|자랑스러운 경찰관 여러분,                                                                                                                       |\n",
      "|                                                                                                                                     |\n",
      "|경찰헌장은 “나라와 겨레를 위하여 충성”을 다한다는 다짐으로 시작합니다. 헌장처럼 우리 경찰은 ‘나라와 겨레를 위한 충성’의 길을 걸으려 노력해 왔습니다.                                               |\n",
      "|                                                                                                                                     |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
