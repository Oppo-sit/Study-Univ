{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ[\"PYLIB\"]=os.path.join(os.environ[\"SPARK_HOME\"],'python','lib')\n",
    "sys.path.insert(0,os.path.join(os.environ[\"PYLIB\"],'py4j-0.10.1-src.zip'))\n",
    "sys.path.insert(0,os.path.join(os.environ[\"PYLIB\"],'pyspark.zip'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "myConf=pyspark.SparkConf() # SparkSession에 필요한 설정을 삽입해서 만듦. 지금은 공란\n",
    "spark = pyspark.sql.SparkSession.builder\\\n",
    "    .master(\"local\")\\\n",
    "    .appName(\"myApp\")\\\n",
    "    .config(\"spark.sql.warehouse.dir\", \"C:\\users\\g312\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "print spark.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "myList=[('1','kim, js',170),\n",
    "        ('1','lee, sm', 175),\n",
    "        ('2','lim, yg',180),\n",
    "        ('2','lee',170)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "myDf=spark.createDataFrame(myList) #내부든 외부든 createDataFrame쓰면 DF가 생성됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _1: string (nullable = true)\n",
      " |-- _2: string (nullable = true)\n",
      " |-- _3: long (nullable = true)\n",
      "\n",
      "[Row(_1=u'1', _2=u'kim, js', _3=170)]\n"
     ]
    }
   ],
   "source": [
    "myDf.printSchema() #앞 두 column은 string ('')으로, 마지막 column은 long으로 자동 지정\n",
    "print myDf.take(1) #column이름이 없으므로 열은 _1, _2와 같이 표시됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(year=u'1', name=u'kim, js', height=170)]\n"
     ]
    }
   ],
   "source": [
    "print spark.createDataFrame(myList, ['year','name','height']).take(1)\n",
    "#이와 같이 열의 이름을 설정하면 출력시에도 열의 이름으로 보여준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+\n",
      "|name|      item|\n",
      "+----+----------+\n",
      "| kim|  espresso|\n",
      "| lee|     latte|\n",
      "| lee| americano|\n",
      "| lim|  affocato|\n",
      "| kim|long black|\n",
      "| lee|  macciato|\n",
      "| lee|  espresso|\n",
      "| lim|     latte|\n",
      "| kim| americano|\n",
      "| lee|  affocato|\n",
      "+----+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "names = [\"kim\",\"lee\",\"lee\",\"lim\"]\n",
    "items = [\"espresso\",\"latte\",\"americano\",\"affocato\",\"long black\",\"macciato\"]\n",
    "df = spark.createDataFrame([(names[i%4], items[i%6]) for i in range(100)],\\\n",
    "                           [\"name\",\"item\"])\n",
    "#위에 있는 이름과 아이템들을 순서대로 100번 반복반환한다.\n",
    "#names[i%4], items[i%6] i->0, 1, 2, 3 and i->0, 1, 2, 3, 4, 5\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|short name|\n",
      "+----------+\n",
      "|       esp|\n",
      "|       lat|\n",
      "|       ame|\n",
      "+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.item.substr(1, 3).alias(\"short name\")).show(3)\n",
    "#substr -> 선택할 문자의 range(현재는 세글자), alias -> 컬럼의 이름"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row # Row 객체를 사용한다.\n",
    "Person = Row('year','name', 'height') #Row안(속성)에는 column의 이름이 들어감.\n",
    "row1=Person('1','kim, js',170)#Person이란 Row 객체에 맞춰 행을 입력."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row1:  1 kim, js\n"
     ]
    }
   ],
   "source": [
    "print \"row1: \", row1.year, row1.name #row.key 방식으로 출력, row[key]로도 출력된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "myRows = [row1, #myRows(DataFrame)에 들어갈 객체로 만들어진 객체를 넣을 수 있다.\n",
    "          Person('1','lee, sm', 175),\n",
    "          Person('2','lim, yg',180),\n",
    "          Person('2','lee',170)] \n",
    "#DataFrame을 List의 형태로 생성하는 방식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "myDf=spark.createDataFrame(myRows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- year: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- height: long (nullable = true)\n",
      "\n",
      "None\n",
      "+----+-------+------+\n",
      "|year|   name|height|\n",
      "+----+-------+------+\n",
      "|   1|kim, js|   170|\n",
      "|   1|lee, sm|   175|\n",
      "|   2|lim, yg|   180|\n",
      "|   2|    lee|   170|\n",
      "+----+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print myDf.printSchema()\n",
    "myDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Schema를 정의해보자.\n",
    "from pyspark.sql.types import StructType, StructField\n",
    "from pyspark.sql.types import StringType, IntegerType\n",
    "mySchema=StructType([\n",
    "    StructField(\"year\", StringType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"height\", IntegerType(), True)\n",
    "])\n",
    "#현재 StructField엔 column의 명칭, 타입, 그리고 Null의 허용여부가 순서대로 입력되어 있다\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- year: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- height: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(year=u'1', name=u'kim, js', height=170)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myDf=spark.createDataFrame(myRows, mySchema) #처음과 다르게 리스트와 스키마를 같이 부름.\n",
    "myDf.printSchema() #결과에서 보이듯이 자동지정된 Long이 아닌 Int형이다.\n",
    "myDf.take(1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rdd로 DF를 생성해보자.\n",
    "from pyspark.sql import Row\n",
    "\n",
    "myList=[('1','kim, js',170),('1','lee, sm', 175),('2','lim, yg',180),('2','lee',170)]\n",
    "myRdd = spark.sparkContext.parallelize(myList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _1: string (nullable = true)\n",
      " |-- _2: string (nullable = true)\n",
      " |-- _3: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rddDf=myRdd.toDF() #toDF 함수로 RDD가 DF로 형변환된다. Schema가 없으므로 자동지정됨.\n",
    "rddDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _1: string (nullable = true)\n",
      " |-- _2: string (nullable = true)\n",
      " |-- _3: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rddDf=spark.createDataFrame(myRdd) #RDD를 생성함수로도 변환시킬 수 있다.\n",
    "rddDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "| _1|     _2|\n",
      "+---+-------+\n",
      "|  1|kim, js|\n",
      "|  2|    lee|\n",
      "+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rddDf.where(rddDf._3 < 175)\\\n",
    "    .select([rddDf._1, rddDf._2])\\\n",
    "    .show() \n",
    "#데이터에서 _3(세번째 column)의 value<175인 row는?\n",
    "#1, 2번 column을 선택\n",
    "#선택된 column과 row만 보여준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "| _1|max(_3)|\n",
      "+---+-------+\n",
      "|  1|    175|\n",
      "|  2|    180|\n",
      "+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rddDf.groupby(rddDf._1).max().show()\n",
    "#이거 어려워 보이는데 쉬움.\n",
    "#일단 DF를 rddDf._1(key)로 그룹핑하고 그룹마다의 value의 최댓값을 골라 이를 출력한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Row 객체로 위를 구현해보자.\n",
    "_myRdd=myRdd.map(lambda x:Row(year=int(x[0]),name=x[1],height=int(x[2])))\n",
    "#year는 int로 자동 형변환된다.\n",
    "#맵에서 column이름을 정해준다.\n",
    "#RDD에서도 Row 객체를 쓸 수 있구나."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- height: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- year: long (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(height=170, name=u'kim, js', year=1)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_myDf=spark.createDataFrame(_myRdd) # 위와 같은 과정\n",
    "_myDf.printSchema() #위에선 int로 받았지만, schema 설정한 것은 아니므로 자동지정\n",
    "_myDf.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import StructType, StructField\n",
    "from pyspark.sql.types import StringType, IntegerType, TimestampType\n",
    "r1=Row(name=\"js1\",age=10)\n",
    "r2=Row(name=\"js2\",age=20)\n",
    "_myRdd=spark.sparkContext.parallelize([r1,r2])\n",
    "#Row 객체로 RDD를 생성할 수 있는 것이었다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(age=10, name='js1'), Row(age=20, name='js2')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_myRdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n",
      "+---+----+\n",
      "|age|name|\n",
      "+---+----+\n",
      "| 10| js1|\n",
      "| 20| js2|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#createDataFrame함수에 (Rdd, schema)로 정의한 schema를 생성할 수 있다.\n",
    "schema=StructType([\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    #StructField(\"created\", TimestampType(), True)\n",
    "])\n",
    "_myDf=spark.createDataFrame(_myRdd,schema)\n",
    "_myDf.printSchema()\n",
    "_myDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- height: double (nullable = true)\n",
      "\n",
      "+---+----+------+\n",
      "| id|name|height|\n",
      "+---+----+------+\n",
      "|  1| kim|  50.0|\n",
      "|  2| lee|  60.0|\n",
      "|  3|park|  70.0|\n",
      "+---+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#위의 RDD->DF + Schema를 정리하면 아래와 같다.\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "myRdd=spark.sparkContext.parallelize([(1, 'kim', 50.0), (2, 'lee', 60.0), (3, 'park', 70.0)])\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"height\", DoubleType(), True)\n",
    "])\n",
    "_myDf = spark.createDataFrame(myRdd, schema)\n",
    "_myDf.printSchema()\n",
    "_myDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>name</th>\n",
       "      <th>height</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>kim, js</td>\n",
       "      <td>170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>lee, sm</td>\n",
       "      <td>175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>lim, yg</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>lee</td>\n",
       "      <td>170</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  year     name  height\n",
       "0    1  kim, js     170\n",
       "1    1  lee, sm     175\n",
       "2    2  lim, yg     180\n",
       "3    2      lee     170"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myDf.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#myDf.write.format('com.databricks.spark.csv').save(os.path.join('data','_myDf.csv')) #error 발생."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!doskey ls = dir #doskey ls란 이름으로 dir 기능을 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile data\\ds_twitter_seoul_3.json\n",
    "{\"contributors\": null, \"truncated\": false, \"text\": \"RT @soompi: #SEVENTEEN’s Mingyu, Jin Se Yeon, And Leeteuk To MC For 2016 Super Seoul Dream Concert \\nhttps://t.co/1XRSaRBbE0 https://t.co/fi…\", \"is_quote_status\": false, \"in_reply_to_status_id\": null, \"id\": 801657325836763136, \"favorite_count\": 0, \"entities\": {\"symbols\": [], \"user_mentions\": [{\"id\": 17659206, \"indices\": [3, 10], \"id_str\": \"17659206\", \"screen_name\": \"soompi\", \"name\": \"Soompi\"}], \"hashtags\": [{\"indices\": [12, 22], \"text\": \"SEVENTEEN\"}], \"urls\": [{\"url\": \"https://t.co/1XRSaRBbE0\", \"indices\": [100, 123], \"expanded_url\": \"http://www.soompi.com/2016/11/20/seventeens-mingyu-jin-se-yeon-leeteuk-mc-dream-concert/\", \"display_url\": \"soompi.com/2016/11/20/sev…\"}]}, \"retweeted\": false, \"coordinates\": null, \"source\": \"<a href=\\\"http://twitter.com/download/android\\\" rel=\\\"nofollow\\\">Twitter for Android</a>\", \"in_reply_to_screen_name\": null, \"in_reply_to_user_id\": null, \"retweet_count\": 1487, \"id_str\": \"801657325836763136\", \"favorited\": false, \"retweeted_status\": {\"contributors\": null, \"truncated\": false, \"text\": \"#SEVENTEEN’s Mingyu, Jin Se Yeon, And Leeteuk To MC For 2016 Super Seoul Dream Concert \\nhttps://t.co/1XRSaRBbE0 https://t.co/fifXHpF8or\", \"is_quote_status\": false, \"in_reply_to_status_id\": null, \"id\": 800593781586132993, \"favorite_count\": 1649, \"entities\": {\"symbols\": [], \"user_mentions\": [], \"hashtags\": [{\"indices\": [0, 10], \"text\": \"SEVENTEEN\"}], \"urls\": [{\"url\": \"https://t.co/1XRSaRBbE0\", \"indices\": [88, 111], \"expanded_url\": \"http://www.soompi.com/2016/11/20/seventeens-mingyu-jin-se-yeon-leeteuk-mc-dream-concert/\", \"display_url\": \"soompi.com/2016/11/20/sev…\"}], \"media\": [{\"expanded_url\": \"https://twitter.com/soompi/status/800593781586132993/photo/1\", \"display_url\": \"pic.twitter.com/fifXHpF8or\", \"url\": \"https://t.co/fifXHpF8or\", \"media_url_https\": \"https://pbs.twimg.com/media/CxxHMk8UsAA4cUT.jpg\", \"id_str\": \"800593115165798400\", \"sizes\": {\"small\": {\"h\": 382, \"resize\": \"fit\", \"w\": 680}, \"large\": {\"h\": 449, \"resize\": \"fit\", \"w\": 800}, \"medium\": {\"h\": 449, \"resize\": \"fit\", \"w\": 800}, \"thumb\": {\"h\": 150, \"resize\": \"crop\", \"w\": 150}}, \"indices\": [112, 135], \"type\": \"photo\", \"id\": 800593115165798400, \"media_url\": \"http://pbs.twimg.com/media/CxxHMk8UsAA4cUT.jpg\"}]}, \"retweeted\": false, \"coordinates\": null, \"source\": \"<a href=\\\"https://about.twitter.com/products/tweetdeck\\\" rel=\\\"nofollow\\\">TweetDeck</a>\", \"in_reply_to_screen_name\": null, \"in_reply_to_user_id\": null, \"retweet_count\": 1487, \"id_str\": \"800593781586132993\", \"favorited\": false, \"user\": {\"follow_request_sent\": false, \"has_extended_profile\": true, \"profile_use_background_image\": true, \"default_profile_image\": false, \"id\": 17659206, \"profile_background_image_url_https\": \"https://pbs.twimg.com/profile_background_images/699864769/1cdde0a85f5c0a994ae1fb06d545a5ec.png\", \"verified\": true, \"translator_type\": \"none\", \"profile_text_color\": \"999999\", \"profile_image_url_https\": \"https://pbs.twimg.com/profile_images/792117259489583104/4khJk3zz_normal.jpg\", \"profile_sidebar_fill_color\": \"000000\", \"entities\": {\"url\": {\"urls\": [{\"url\": \"http://t.co/3evT80UlR9\", \"indices\": [0, 22], \"expanded_url\": \"http://www.soompi.com\", \"display_url\": \"soompi.com\"}]}, \"description\": {\"urls\": []}}, \"followers_count\": 987867, \"profile_sidebar_border_color\": \"000000\", \"id_str\": \"17659206\", \"profile_background_color\": \"1E1E1E\", \"listed_count\": 3982, \"is_translation_enabled\": true, \"utc_offset\": -28800, \"statuses_count\": 80038, \"description\": \"The original K-pop community. We take gifs, OTPs, and reporting on your bias' fashion choices seriously. But not rumors. Ain't nobody got time for that.\", \"friends_count\": 3532, \"location\": \"Worldwide\", \"profile_link_color\": \"31B6F4\", \"profile_image_url\": \"http://pbs.twimg.com/profile_images/792117259489583104/4khJk3zz_normal.jpg\", \"following\": false, \"geo_enabled\": false, \"profile_banner_url\": \"https://pbs.twimg.com/profile_banners/17659206/1478803767\", \"profile_background_image_url\": \"http://pbs.twimg.com/profile_background_images/699864769/1cdde0a85f5c0a994ae1fb06d545a5ec.png\", \"screen_name\": \"soompi\", \"lang\": \"en\", \"profile_background_tile\": true, \"favourites_count\": 1493, \"name\": \"Soompi\", \"notifications\": false, \"url\": \"http://t.co/3evT80UlR9\", \"created_at\": \"Wed Nov 26 20:48:27 +0000 2008\", \"contributors_enabled\": false, \"time_zone\": \"Pacific Time (US & Canada)\", \"protected\": false, \"default_profile\": false, \"is_translator\": false}, \"geo\": null, \"in_reply_to_user_id_str\": null, \"possibly_sensitive\": false, \"lang\": \"en\", \"created_at\": \"Mon Nov 21 06:56:46 +0000 2016\", \"in_reply_to_status_id_str\": null, \"place\": null, \"extended_entities\": {\"media\": [{\"expanded_url\": \"https://twitter.com/soompi/status/800593781586132993/photo/1\", \"display_url\": \"pic.twitter.com/fifXHpF8or\", \"url\": \"https://t.co/fifXHpF8or\", \"media_url_https\": \"https://pbs.twimg.com/media/CxxHMk8UsAA4cUT.jpg\", \"id_str\": \"800593115165798400\", \"sizes\": {\"small\": {\"h\": 382, \"resize\": \"fit\", \"w\": 680}, \"large\": {\"h\": 449, \"resize\": \"fit\", \"w\": 800}, \"medium\": {\"h\": 449, \"resize\": \"fit\", \"w\": 800}, \"thumb\": {\"h\": 150, \"resize\": \"crop\", \"w\": 150}}, \"indices\": [112, 135], \"type\": \"photo\", \"id\": 800593115165798400, \"media_url\": \"http://pbs.twimg.com/media/CxxHMk8UsAA4cUT.jpg\"}]}, \"metadata\": {\"iso_language_code\": \"en\", \"result_type\": \"recent\"}}, \"user\": {\"follow_request_sent\": false, \"has_extended_profile\": false, \"profile_use_background_image\": true, \"default_profile_image\": true, \"id\": 791090169818521600, \"profile_background_image_url_https\": null, \"verified\": false, \"translator_type\": \"none\", \"profile_text_color\": \"333333\", \"profile_image_url_https\": \"https://abs.twimg.com/sticky/default_profile_images/default_profile_6_normal.png\", \"profile_sidebar_fill_color\": \"DDEEF6\", \"entities\": {\"description\": {\"urls\": []}}, \"followers_count\": 0, \"profile_sidebar_border_color\": \"C0DEED\", \"id_str\": \"791090169818521600\", \"profile_background_color\": \"F5F8FA\", \"listed_count\": 0, \"is_translation_enabled\": false, \"utc_offset\": null, \"statuses_count\": 96, \"description\": \"\", \"friends_count\": 7, \"location\": \"\", \"profile_link_color\": \"1DA1F2\", \"profile_image_url\": \"http://abs.twimg.com/sticky/default_profile_images/default_profile_6_normal.png\", \"following\": false, \"geo_enabled\": false, \"profile_background_image_url\": null, \"screen_name\": \"enriquesanq\", \"lang\": \"es\", \"profile_background_tile\": false, \"favourites_count\": 161, \"name\": \"Enrique santos\", \"notifications\": false, \"url\": null, \"created_at\": \"Wed Oct 26 01:32:49 +0000 2016\", \"contributors_enabled\": false, \"time_zone\": null, \"protected\": false, \"default_profile\": true, \"is_translator\": false}, \"geo\": null, \"in_reply_to_user_id_str\": null, \"possibly_sensitive\": false, \"lang\": \"en\", \"created_at\": \"Thu Nov 24 05:22:55 +0000 2016\", \"in_reply_to_status_id_str\": null, \"place\": null, \"metadata\": {\"iso_language_code\": \"en\", \"result_type\": \"recent\"}}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "_jfname=os.path.join('data','ds_twitter_seoul_3.json')\n",
    "with open(_jfname, 'rb') as f:\n",
    "    data = f.readlines()\n",
    "    \n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = map(lambda x: x.rstrip(), data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print \"+\".join([\"A\", \"B\", \"C\"]) #join 함수는 앞의 문자와 안의 인자를 결합\n",
    "\n",
    "data_json_str = \"[\" + ','.join(data) + \"]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_json_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_df = pd.read_json(data_json_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print data_df.count() #행의 개수  #전자는 column명이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df['id'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "bicycleRdd=spark.sparkContext.textFile(os.path.join('data','bicycle.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'\\ufffd\\ubfe9\\ufffd\\ufffd\\ufffd\\ufffd,\\ufffd\\ubfe9\\ufffd\\u01fc\\ufffd',\n",
       " u'2018-01-01,4950',\n",
       " u'2018-01-02,7136',\n",
       " u'2018-01-03,7156',\n",
       " u'2018-01-04,7102',\n",
       " u'2018-01-05,7705',\n",
       " u'2018-01-06,5681',\n",
       " u'2018-01-07,5220',\n",
       " u'2018-01-08,6309',\n",
       " u'2018-01-09,5988',\n",
       " u'2018-01-10,4476',\n",
       " u'2018-01-11,4337',\n",
       " u'2018-01-12,4401',\n",
       " u'2018-01-13,3756',\n",
       " u'2018-01-14,4675',\n",
       " u'2018-01-15,6993',\n",
       " u'2018-01-16,7421',\n",
       " u'2018-01-17,6990',\n",
       " u'2018-01-18,7054',\n",
       " u'2018-01-19,8329',\n",
       " u'2018-01-20,6148',\n",
       " u'2018-01-21,5574',\n",
       " u'2018-01-22,4929',\n",
       " u'2018-01-23,4263',\n",
       " u'2018-01-24,3370',\n",
       " u'2018-01-25,3307',\n",
       " u'2018-01-26,3116',\n",
       " u'2018-01-27,2833',\n",
       " u'2018-01-28,3028',\n",
       " u'2018-01-29,4425',\n",
       " u'2018-01-30,3591',\n",
       " u'2018-01-31,4104',\n",
       " u'2018-02-01,5821',\n",
       " u'2018-02-02,6557',\n",
       " u'2018-02-03,3499',\n",
       " u'2018-02-04,2642',\n",
       " u'2018-02-05,4213',\n",
       " u'2018-02-06,4257',\n",
       " u'2018-02-07,5028',\n",
       " u'2018-02-08,6114',\n",
       " u'2018-02-09,6905',\n",
       " u'2018-02-10,4829',\n",
       " u'2018-02-11,3047',\n",
       " u'2018-02-12,5231',\n",
       " u'2018-02-13,6863',\n",
       " u'2018-02-14,7477',\n",
       " u'2018-02-15,3935',\n",
       " u'2018-02-16,3047',\n",
       " u'2018-02-17,3494',\n",
       " u'2018-02-18,5191',\n",
       " u'2018-02-19,8563',\n",
       " u'2018-02-20,8899',\n",
       " u'2018-02-21,8342',\n",
       " u'2018-02-22,8580',\n",
       " u'2018-02-23,6408',\n",
       " u'2018-02-24,7198',\n",
       " u'2018-02-25,7595',\n",
       " u'2018-02-26,10712',\n",
       " u'2018-02-27,10064',\n",
       " u'2018-02-28,4230',\n",
       " u'2018-03-01,5761',\n",
       " u'2018-03-02,9388',\n",
       " u'2018-03-03,16029',\n",
       " u'2018-03-04,5373',\n",
       " u'2018-03-05,9943',\n",
       " u'2018-03-06,14028',\n",
       " u'2018-03-07,12925',\n",
       " u'2018-03-08,8812',\n",
       " u'2018-03-09,13573',\n",
       " u'2018-03-10,13324',\n",
       " u'2018-03-11,15136',\n",
       " u'2018-03-12,15913',\n",
       " u'2018-03-13,19514',\n",
       " u'2018-03-14,24911',\n",
       " u'2018-03-15,7091',\n",
       " u'2018-03-16,18415',\n",
       " u'2018-03-17,20165',\n",
       " u'2018-03-18,11740',\n",
       " u'2018-03-19,10297',\n",
       " u'2018-03-20,13205',\n",
       " u'2018-03-21,8225',\n",
       " u'2018-03-22,16420',\n",
       " u'2018-03-23,17904',\n",
       " u'2018-03-24,14914',\n",
       " u'2018-03-25,13942',\n",
       " u'2018-03-26,15505',\n",
       " u'2018-03-27,17456',\n",
       " u'2018-03-28,21232',\n",
       " u'2018-03-29,21436',\n",
       " u'2018-03-30,24411',\n",
       " u'2018-03-31,25673',\n",
       " u'2018-04-01,22994',\n",
       " u'2018-04-02,28139',\n",
       " u'2018-04-03,26817',\n",
       " u'2018-04-04,26034',\n",
       " u'2018-04-05,2833',\n",
       " u'2018-04-06,13297',\n",
       " u'2018-04-07,13088',\n",
       " u'2018-04-08,7758',\n",
       " u'2018-04-09,23925',\n",
       " u'2018-04-10,20423',\n",
       " u'2018-04-11,25557',\n",
       " u'2018-04-12,28885',\n",
       " u'2018-04-13,29497',\n",
       " u'2018-04-14,8216',\n",
       " u'2018-04-15,12721',\n",
       " u'2018-04-16,29589',\n",
       " u'2018-04-17,28236',\n",
       " u'2018-04-18,23499',\n",
       " u'2018-04-19,20400',\n",
       " u'2018-04-20,28351',\n",
       " u'2018-04-21,33144',\n",
       " u'2018-04-22,8095',\n",
       " u'2018-04-23,1037',\n",
       " u'2018-04-24,24560',\n",
       " u'2018-04-25,33728',\n",
       " u'2018-04-26,32547',\n",
       " u'2018-04-27,32989',\n",
       " u'2018-04-28,33738',\n",
       " u'2018-04-29,35159',\n",
       " u'2018-04-30,32629',\n",
       " u'2018-05-01,34593',\n",
       " u'2018-05-02,8374',\n",
       " u'2018-05-03,25963',\n",
       " u'2018-05-04,34058',\n",
       " u'2018-05-05,37389',\n",
       " u'2018-05-06,19809',\n",
       " u'2018-05-07,40026',\n",
       " u'2018-05-08,33738',\n",
       " u'2018-05-09,37851',\n",
       " u'2018-05-10,36549',\n",
       " u'2018-05-11,33923',\n",
       " u'2018-05-12,3551',\n",
       " u'2018-05-13,35668',\n",
       " u'2018-05-14,35918',\n",
       " u'2018-05-15,36476',\n",
       " u'2018-05-16,7072',\n",
       " u'2018-05-17,3802',\n",
       " u'2018-05-18,19629',\n",
       " u'2018-05-19,43471',\n",
       " u'2018-05-20,39036',\n",
       " u'2018-05-21,42039',\n",
       " u'2018-05-22,17444',\n",
       " u'2018-05-23,35975',\n",
       " u'2018-05-24,36858',\n",
       " u'2018-05-25,37569',\n",
       " u'2018-05-26,37761',\n",
       " u'2018-05-27,39534',\n",
       " u'2018-05-28,40775',\n",
       " u'2018-05-29,29925',\n",
       " u'2018-05-30,39214',\n",
       " u'2018-05-31,41619',\n",
       " u'2018-06-01,41970',\n",
       " u'2018-06-02,41677',\n",
       " u'2018-06-03,40565',\n",
       " u'2018-06-04,44014',\n",
       " u'2018-06-05,44118',\n",
       " u'2018-06-06,40440',\n",
       " u'2018-06-07,42038',\n",
       " u'2018-06-08,46516',\n",
       " u'2018-06-09,32839',\n",
       " u'2018-06-10,39277',\n",
       " u'2018-06-11,30605',\n",
       " u'2018-06-12,46201',\n",
       " u'2018-06-13,49116',\n",
       " u'2018-06-14,30511',\n",
       " u'2018-06-15,45103',\n",
       " u'2018-06-16,46457',\n",
       " u'2018-06-17,43484',\n",
       " u'2018-06-18,36316',\n",
       " u'2018-06-19,47296',\n",
       " u'2018-06-20,46612',\n",
       " u'2018-06-21,47762',\n",
       " u'2018-06-22,47478',\n",
       " u'2018-06-23,45663',\n",
       " u'2018-06-24,39548',\n",
       " u'2018-06-25,46565',\n",
       " u'2018-06-26,6391',\n",
       " u'2018-06-27,41128',\n",
       " u'2018-06-28,27757',\n",
       " u'2018-06-29,42765',\n",
       " u'2018-06-30,26911',\n",
       " u'2018-07-01,4358',\n",
       " u'2018-07-02,6261',\n",
       " u'2018-07-03,41350',\n",
       " u'2018-07-04,45741',\n",
       " u'2018-07-05,39804',\n",
       " u'2018-07-06,49521',\n",
       " u'2018-07-07,48015',\n",
       " u'2018-07-08,48132',\n",
       " u'2018-07-09,8155',\n",
       " u'2018-07-10,27956',\n",
       " u'2018-07-11,35390',\n",
       " u'2018-07-12,43611',\n",
       " u'2018-07-13,44976',\n",
       " u'2018-07-14,39205',\n",
       " u'2018-07-15,34957',\n",
       " u'2018-07-16,40964',\n",
       " u'2018-07-17,43323',\n",
       " u'2018-07-18,42609',\n",
       " u'2018-07-19,42316',\n",
       " u'2018-07-20,40337',\n",
       " u'2018-07-21,32360',\n",
       " u'2018-07-22,28250',\n",
       " u'2018-07-23,38003',\n",
       " u'2018-07-24,37486',\n",
       " u'2018-07-25,39225',\n",
       " u'2018-07-26,37796',\n",
       " u'2018-07-27,34985',\n",
       " u'2018-07-28,24649',\n",
       " u'2018-07-29,28317',\n",
       " u'2018-07-30,36701',\n",
       " u'2018-07-31,35262',\n",
       " u'2018-08-01,31169',\n",
       " u'2018-08-02,31321',\n",
       " u'2018-08-03,31646',\n",
       " u'2018-08-04,28174',\n",
       " u'2018-08-05,26989',\n",
       " u'2018-08-06,29113',\n",
       " u'2018-08-07,36983',\n",
       " u'2018-08-08,37140',\n",
       " u'2018-08-09,32216',\n",
       " u'2018-08-10,36178',\n",
       " u'2018-08-11,31293',\n",
       " u'2018-08-12,27363',\n",
       " u'2018-08-13,36076',\n",
       " u'2018-08-14,37659',\n",
       " u'2018-08-15,29270',\n",
       " u'2018-08-16,41358',\n",
       " u'2018-08-17,47634',\n",
       " u'2018-08-18,45629',\n",
       " u'2018-08-19,42281',\n",
       " u'2018-08-20,44243',\n",
       " u'2018-08-21,41964',\n",
       " u'2018-08-22,42425',\n",
       " u'2018-08-23,23616',\n",
       " u'2018-08-24,17319',\n",
       " u'2018-08-25,47109',\n",
       " u'2018-08-26,32200',\n",
       " u'2018-08-27,23382',\n",
       " u'2018-08-28,13495',\n",
       " u'2018-08-29,13001',\n",
       " u'2018-08-30,33158',\n",
       " u'2018-08-31,46101',\n",
       " u'2018-09-01,43847',\n",
       " u'2018-09-02,48427',\n",
       " u'2018-09-03,16086',\n",
       " u'2018-09-04,50500',\n",
       " u'2018-09-05,53984',\n",
       " u'2018-09-06,46532',\n",
       " u'2018-09-07,53230',\n",
       " u'2018-09-08,55470',\n",
       " u'2018-09-09,54213',\n",
       " u'2018-09-10,55188',\n",
       " u'2018-09-11,56212',\n",
       " u'2018-09-12,57363',\n",
       " u'2018-09-13,55791',\n",
       " u'2018-09-14,49383',\n",
       " u'2018-09-15,44808',\n",
       " u'2018-09-16,24317',\n",
       " u'2018-09-17,55335',\n",
       " u'2018-09-18,57691',\n",
       " u'2018-09-19,44096',\n",
       " u'2018-09-20,23616',\n",
       " u'2018-09-21,32242',\n",
       " u'2018-09-22,49688',\n",
       " u'2018-09-23,38635',\n",
       " u'2018-09-24,36406',\n",
       " u'2018-09-25,49595',\n",
       " u'2018-09-26,57730',\n",
       " u'2018-09-27,58200',\n",
       " u'2018-09-28,59649',\n",
       " u'2018-09-29,63866',\n",
       " u'2018-09-30,55893',\n",
       " u'2018-10-01,51808',\n",
       " u'2018-10-02,57280',\n",
       " u'2018-10-03,64671',\n",
       " u'2018-10-04,59413',\n",
       " u'2018-10-05,7772',\n",
       " u'2018-10-06,32510',\n",
       " u'2018-10-07,57873',\n",
       " u'2018-10-08,57247',\n",
       " u'2018-10-09,61525',\n",
       " u'2018-10-10,44876',\n",
       " u'2018-10-11,44489',\n",
       " u'2018-10-12,48529',\n",
       " u'2018-10-13,49203',\n",
       " u'2018-10-14,49025',\n",
       " u'2018-10-15,49503',\n",
       " u'2018-10-16,51236',\n",
       " u'2018-10-17,49795',\n",
       " u'2018-10-18,49560',\n",
       " u'2018-10-19,52122',\n",
       " u'2018-10-20,49815',\n",
       " u'2018-10-21,48346',\n",
       " u'2018-10-22,49889',\n",
       " u'2018-10-23,40223',\n",
       " u'2018-10-24,50343',\n",
       " u'2018-10-25,52181',\n",
       " u'2018-10-26,24308',\n",
       " u'2018-10-27,33699',\n",
       " u'2018-10-28,20882',\n",
       " u'2018-10-29,36107',\n",
       " u'2018-10-30,37382',\n",
       " u'2018-10-31,39009',\n",
       " u'2018-11-01,41911',\n",
       " u'2018-11-02,44917',\n",
       " u'2018-11-03,40640',\n",
       " u'2018-11-04,39973',\n",
       " u'2018-11-05,43870',\n",
       " u'2018-11-06,39680',\n",
       " u'2018-11-07,29492',\n",
       " u'2018-11-08,2730',\n",
       " u'2018-11-09,34673',\n",
       " u'2018-11-10,34194',\n",
       " u'2018-11-11,23926',\n",
       " u'2018-11-12,38706',\n",
       " u'2018-11-13,40263',\n",
       " u'2018-11-14,41973',\n",
       " u'2018-11-15,39702',\n",
       " u'2018-11-16,37629',\n",
       " u'2018-11-17,28167',\n",
       " u'2018-11-18,26464',\n",
       " u'2018-11-19,34352',\n",
       " u'2018-11-20,34855',\n",
       " u'2018-11-21,28140',\n",
       " u'2018-11-22,29239',\n",
       " u'2018-11-23,28620',\n",
       " u'2018-11-24,11255',\n",
       " u'2018-11-25,19946',\n",
       " u'2018-11-26,30698',\n",
       " u'2018-11-27,28806',\n",
       " u'2018-11-28,29177',\n",
       " u'2018-11-29,28977',\n",
       " u'2018-11-30,28557',\n",
       " u'2018-12-01,24622',\n",
       " u'2018-12-02,23035',\n",
       " u'2018-12-03,10638',\n",
       " u'2018-12-04,20029',\n",
       " u'2018-12-05,23535',\n",
       " u'2018-12-06,23788',\n",
       " u'2018-12-07,15148',\n",
       " u'2018-12-08,9824',\n",
       " u'2018-12-09,8749',\n",
       " u'2018-12-10,17987',\n",
       " u'2018-12-11,20805',\n",
       " u'2018-12-12,19339',\n",
       " u'2018-12-13,11765',\n",
       " u'2018-12-14,14932',\n",
       " u'2018-12-15,12720',\n",
       " u'2018-12-16,10783',\n",
       " u'2018-12-17,17449',\n",
       " u'2018-12-18,20917',\n",
       " u'2018-12-19,21970',\n",
       " u'2018-12-20,22881',\n",
       " u'2018-12-21,23422',\n",
       " u'2018-12-22,17127',\n",
       " u'2018-12-23,13601',\n",
       " u'2018-12-24,16579',\n",
       " u'2018-12-25,12437',\n",
       " u'2018-12-26,17112',\n",
       " u'2018-12-27,11325',\n",
       " u'2018-12-28,10445',\n",
       " u'2018-12-29,8083',\n",
       " u'2018-12-30,7864',\n",
       " u'2018-12-31,11911',\n",
       " u'2019-01-01,8601',\n",
       " u'2019-01-02,14360',\n",
       " u'2019-01-03,16344',\n",
       " u'2019-01-04,17308',\n",
       " u'2019-01-05,12380',\n",
       " u'2019-01-06,11094',\n",
       " u'2019-01-07,16795',\n",
       " u'2019-01-08,15833',\n",
       " u'2019-01-09,15819',\n",
       " u'2019-01-10,17894',\n",
       " u'2019-01-11,19271',\n",
       " u'2019-01-12,14519',\n",
       " u'2019-01-13,11751',\n",
       " u'2019-01-14,14818',\n",
       " u'2019-01-15,14761',\n",
       " u'2019-01-16,15823',\n",
       " u'2019-01-17,17960',\n",
       " u'2019-01-18,18901',\n",
       " u'2019-01-19,14883',\n",
       " u'2019-01-20,11191',\n",
       " u'2019-01-21,17240',\n",
       " u'2019-01-22,19885',\n",
       " u'2019-01-23,19163',\n",
       " u'2019-01-24,20057',\n",
       " u'2019-01-25,18159',\n",
       " u'2019-01-26,13203',\n",
       " u'2019-01-27,12440',\n",
       " u'2019-01-28,17317',\n",
       " u'2019-01-29,19477',\n",
       " u'2019-01-30,20677',\n",
       " u'2019-01-31,17649',\n",
       " u'2019-02-01,17696',\n",
       " u'2019-02-02,13742',\n",
       " u'2019-02-03,2010',\n",
       " u'2019-02-04,8562',\n",
       " u'2019-02-05,9119',\n",
       " u'2019-02-06,13694',\n",
       " u'2019-02-07,14533',\n",
       " u'2019-02-08,14782',\n",
       " u'2019-02-09,11760',\n",
       " u'2019-02-10,9759',\n",
       " u'2019-02-11,16881',\n",
       " u'2019-02-12,18862',\n",
       " u'2019-02-13,18411',\n",
       " u'2019-02-14,19376',\n",
       " u'2019-02-15,9213',\n",
       " u'2019-02-16,12641',\n",
       " u'2019-02-17,13390',\n",
       " u'2019-02-18,19916',\n",
       " u'2019-02-19,11689',\n",
       " u'2019-02-20,19572',\n",
       " u'2019-02-21,20995',\n",
       " u'2019-02-22,21727',\n",
       " u'2019-02-23,23742',\n",
       " u'2019-02-24,21933',\n",
       " u'2019-02-25,23445',\n",
       " u'2019-02-26,28010',\n",
       " u'2019-02-27,28652',\n",
       " u'2019-02-28,27431',\n",
       " u'2019-03-01,21751',\n",
       " u'2019-03-02,22503',\n",
       " u'2019-03-03,24382',\n",
       " u'2019-03-04,26887',\n",
       " u'2019-03-05,24042',\n",
       " u'2019-03-06,25379',\n",
       " u'2019-03-07,31156',\n",
       " u'2019-03-08,33073',\n",
       " u'2019-03-09,32421',\n",
       " u'2019-03-10,31050',\n",
       " u'2019-03-11,28539',\n",
       " u'2019-03-12,25537',\n",
       " u'2019-03-13,28653',\n",
       " u'2019-03-14,30997',\n",
       " u'2019-03-15,20481',\n",
       " u'2019-03-16,26057',\n",
       " u'2019-03-17,35622',\n",
       " u'2019-03-18,36313',\n",
       " u'2019-03-19,39908',\n",
       " u'2019-03-20,16204',\n",
       " u'2019-03-21,26455',\n",
       " u'2019-03-22,32363',\n",
       " u'2019-03-23,21696',\n",
       " u'2019-03-24,31622',\n",
       " u'2019-03-25,34226',\n",
       " u'2019-03-26,42644',\n",
       " u'2019-03-27,35832',\n",
       " u'2019-03-28,38806',\n",
       " u'2019-03-29,43893',\n",
       " u'2019-03-30,14001',\n",
       " u'2019-03-31,22326']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bicycleRdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField\n",
    "from pyspark.sql.types import StringType, IntegerType\n",
    "bicycleSchema=StructType([\n",
    "    StructField(\"date\", StringType(), True),\n",
    "    StructField(\"count\", IntegerType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "bicycleRdd=bicycleRdd.map(lambda x:x.split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'2018-01-01', u'4950'],\n",
       " [u'2018-01-02', u'7136'],\n",
       " [u'2018-01-03', u'7156'],\n",
       " [u'2018-01-04', u'7102'],\n",
       " [u'2018-01-05', u'7705'],\n",
       " [u'2018-01-06', u'5681'],\n",
       " [u'2018-01-07', u'5220'],\n",
       " [u'2018-01-08', u'6309'],\n",
       " [u'2018-01-09', u'5988'],\n",
       " [u'2018-01-10', u'4476'],\n",
       " [u'2018-01-11', u'4337'],\n",
       " [u'2018-01-12', u'4401'],\n",
       " [u'2018-01-13', u'3756'],\n",
       " [u'2018-01-14', u'4675'],\n",
       " [u'2018-01-15', u'6993'],\n",
       " [u'2018-01-16', u'7421'],\n",
       " [u'2018-01-17', u'6990'],\n",
       " [u'2018-01-18', u'7054'],\n",
       " [u'2018-01-19', u'8329'],\n",
       " [u'2018-01-20', u'6148'],\n",
       " [u'2018-01-21', u'5574'],\n",
       " [u'2018-01-22', u'4929'],\n",
       " [u'2018-01-23', u'4263'],\n",
       " [u'2018-01-24', u'3370'],\n",
       " [u'2018-01-25', u'3307'],\n",
       " [u'2018-01-26', u'3116'],\n",
       " [u'2018-01-27', u'2833'],\n",
       " [u'2018-01-28', u'3028'],\n",
       " [u'2018-01-29', u'4425'],\n",
       " [u'2018-01-30', u'3591'],\n",
       " [u'2018-01-31', u'4104'],\n",
       " [u'2018-02-01', u'5821'],\n",
       " [u'2018-02-02', u'6557'],\n",
       " [u'2018-02-03', u'3499'],\n",
       " [u'2018-02-04', u'2642'],\n",
       " [u'2018-02-05', u'4213'],\n",
       " [u'2018-02-06', u'4257'],\n",
       " [u'2018-02-07', u'5028'],\n",
       " [u'2018-02-08', u'6114'],\n",
       " [u'2018-02-09', u'6905'],\n",
       " [u'2018-02-10', u'4829'],\n",
       " [u'2018-02-11', u'3047'],\n",
       " [u'2018-02-12', u'5231'],\n",
       " [u'2018-02-13', u'6863'],\n",
       " [u'2018-02-14', u'7477'],\n",
       " [u'2018-02-15', u'3935'],\n",
       " [u'2018-02-16', u'3047'],\n",
       " [u'2018-02-17', u'3494'],\n",
       " [u'2018-02-18', u'5191'],\n",
       " [u'2018-02-19', u'8563'],\n",
       " [u'2018-02-20', u'8899'],\n",
       " [u'2018-02-21', u'8342'],\n",
       " [u'2018-02-22', u'8580'],\n",
       " [u'2018-02-23', u'6408'],\n",
       " [u'2018-02-24', u'7198'],\n",
       " [u'2018-02-25', u'7595'],\n",
       " [u'2018-02-26', u'10712'],\n",
       " [u'2018-02-27', u'10064'],\n",
       " [u'2018-02-28', u'4230'],\n",
       " [u'2018-03-01', u'5761'],\n",
       " [u'2018-03-02', u'9388'],\n",
       " [u'2018-03-03', u'16029'],\n",
       " [u'2018-03-04', u'5373'],\n",
       " [u'2018-03-05', u'9943'],\n",
       " [u'2018-03-06', u'14028'],\n",
       " [u'2018-03-07', u'12925'],\n",
       " [u'2018-03-08', u'8812'],\n",
       " [u'2018-03-09', u'13573'],\n",
       " [u'2018-03-10', u'13324'],\n",
       " [u'2018-03-11', u'15136'],\n",
       " [u'2018-03-12', u'15913'],\n",
       " [u'2018-03-13', u'19514'],\n",
       " [u'2018-03-14', u'24911'],\n",
       " [u'2018-03-15', u'7091'],\n",
       " [u'2018-03-16', u'18415'],\n",
       " [u'2018-03-17', u'20165'],\n",
       " [u'2018-03-18', u'11740'],\n",
       " [u'2018-03-19', u'10297'],\n",
       " [u'2018-03-20', u'13205'],\n",
       " [u'2018-03-21', u'8225'],\n",
       " [u'2018-03-22', u'16420'],\n",
       " [u'2018-03-23', u'17904'],\n",
       " [u'2018-03-24', u'14914'],\n",
       " [u'2018-03-25', u'13942'],\n",
       " [u'2018-03-26', u'15505'],\n",
       " [u'2018-03-27', u'17456'],\n",
       " [u'2018-03-28', u'21232'],\n",
       " [u'2018-03-29', u'21436'],\n",
       " [u'2018-03-30', u'24411'],\n",
       " [u'2018-03-31', u'25673'],\n",
       " [u'2018-04-01', u'22994'],\n",
       " [u'2018-04-02', u'28139'],\n",
       " [u'2018-04-03', u'26817'],\n",
       " [u'2018-04-04', u'26034'],\n",
       " [u'2018-04-05', u'2833'],\n",
       " [u'2018-04-06', u'13297'],\n",
       " [u'2018-04-07', u'13088'],\n",
       " [u'2018-04-08', u'7758'],\n",
       " [u'2018-04-09', u'23925'],\n",
       " [u'2018-04-10', u'20423'],\n",
       " [u'2018-04-11', u'25557'],\n",
       " [u'2018-04-12', u'28885'],\n",
       " [u'2018-04-13', u'29497'],\n",
       " [u'2018-04-14', u'8216'],\n",
       " [u'2018-04-15', u'12721'],\n",
       " [u'2018-04-16', u'29589'],\n",
       " [u'2018-04-17', u'28236'],\n",
       " [u'2018-04-18', u'23499'],\n",
       " [u'2018-04-19', u'20400'],\n",
       " [u'2018-04-20', u'28351'],\n",
       " [u'2018-04-21', u'33144'],\n",
       " [u'2018-04-22', u'8095'],\n",
       " [u'2018-04-23', u'1037'],\n",
       " [u'2018-04-24', u'24560'],\n",
       " [u'2018-04-25', u'33728'],\n",
       " [u'2018-04-26', u'32547'],\n",
       " [u'2018-04-27', u'32989'],\n",
       " [u'2018-04-28', u'33738'],\n",
       " [u'2018-04-29', u'35159'],\n",
       " [u'2018-04-30', u'32629'],\n",
       " [u'2018-05-01', u'34593'],\n",
       " [u'2018-05-02', u'8374'],\n",
       " [u'2018-05-03', u'25963'],\n",
       " [u'2018-05-04', u'34058'],\n",
       " [u'2018-05-05', u'37389'],\n",
       " [u'2018-05-06', u'19809'],\n",
       " [u'2018-05-07', u'40026'],\n",
       " [u'2018-05-08', u'33738'],\n",
       " [u'2018-05-09', u'37851'],\n",
       " [u'2018-05-10', u'36549'],\n",
       " [u'2018-05-11', u'33923'],\n",
       " [u'2018-05-12', u'3551'],\n",
       " [u'2018-05-13', u'35668'],\n",
       " [u'2018-05-14', u'35918'],\n",
       " [u'2018-05-15', u'36476'],\n",
       " [u'2018-05-16', u'7072'],\n",
       " [u'2018-05-17', u'3802'],\n",
       " [u'2018-05-18', u'19629'],\n",
       " [u'2018-05-19', u'43471'],\n",
       " [u'2018-05-20', u'39036'],\n",
       " [u'2018-05-21', u'42039'],\n",
       " [u'2018-05-22', u'17444'],\n",
       " [u'2018-05-23', u'35975'],\n",
       " [u'2018-05-24', u'36858'],\n",
       " [u'2018-05-25', u'37569'],\n",
       " [u'2018-05-26', u'37761'],\n",
       " [u'2018-05-27', u'39534'],\n",
       " [u'2018-05-28', u'40775'],\n",
       " [u'2018-05-29', u'29925'],\n",
       " [u'2018-05-30', u'39214'],\n",
       " [u'2018-05-31', u'41619'],\n",
       " [u'2018-06-01', u'41970'],\n",
       " [u'2018-06-02', u'41677'],\n",
       " [u'2018-06-03', u'40565'],\n",
       " [u'2018-06-04', u'44014'],\n",
       " [u'2018-06-05', u'44118'],\n",
       " [u'2018-06-06', u'40440'],\n",
       " [u'2018-06-07', u'42038'],\n",
       " [u'2018-06-08', u'46516'],\n",
       " [u'2018-06-09', u'32839'],\n",
       " [u'2018-06-10', u'39277'],\n",
       " [u'2018-06-11', u'30605'],\n",
       " [u'2018-06-12', u'46201'],\n",
       " [u'2018-06-13', u'49116'],\n",
       " [u'2018-06-14', u'30511'],\n",
       " [u'2018-06-15', u'45103'],\n",
       " [u'2018-06-16', u'46457'],\n",
       " [u'2018-06-17', u'43484'],\n",
       " [u'2018-06-18', u'36316'],\n",
       " [u'2018-06-19', u'47296'],\n",
       " [u'2018-06-20', u'46612'],\n",
       " [u'2018-06-21', u'47762'],\n",
       " [u'2018-06-22', u'47478'],\n",
       " [u'2018-06-23', u'45663'],\n",
       " [u'2018-06-24', u'39548'],\n",
       " [u'2018-06-25', u'46565'],\n",
       " [u'2018-06-26', u'6391'],\n",
       " [u'2018-06-27', u'41128'],\n",
       " [u'2018-06-28', u'27757'],\n",
       " [u'2018-06-29', u'42765'],\n",
       " [u'2018-06-30', u'26911'],\n",
       " [u'2018-07-01', u'4358'],\n",
       " [u'2018-07-02', u'6261'],\n",
       " [u'2018-07-03', u'41350'],\n",
       " [u'2018-07-04', u'45741'],\n",
       " [u'2018-07-05', u'39804'],\n",
       " [u'2018-07-06', u'49521'],\n",
       " [u'2018-07-07', u'48015'],\n",
       " [u'2018-07-08', u'48132'],\n",
       " [u'2018-07-09', u'8155'],\n",
       " [u'2018-07-10', u'27956'],\n",
       " [u'2018-07-11', u'35390'],\n",
       " [u'2018-07-12', u'43611'],\n",
       " [u'2018-07-13', u'44976'],\n",
       " [u'2018-07-14', u'39205'],\n",
       " [u'2018-07-15', u'34957'],\n",
       " [u'2018-07-16', u'40964'],\n",
       " [u'2018-07-17', u'43323'],\n",
       " [u'2018-07-18', u'42609'],\n",
       " [u'2018-07-19', u'42316'],\n",
       " [u'2018-07-20', u'40337'],\n",
       " [u'2018-07-21', u'32360'],\n",
       " [u'2018-07-22', u'28250'],\n",
       " [u'2018-07-23', u'38003'],\n",
       " [u'2018-07-24', u'37486'],\n",
       " [u'2018-07-25', u'39225'],\n",
       " [u'2018-07-26', u'37796'],\n",
       " [u'2018-07-27', u'34985'],\n",
       " [u'2018-07-28', u'24649'],\n",
       " [u'2018-07-29', u'28317'],\n",
       " [u'2018-07-30', u'36701'],\n",
       " [u'2018-07-31', u'35262'],\n",
       " [u'2018-08-01', u'31169'],\n",
       " [u'2018-08-02', u'31321'],\n",
       " [u'2018-08-03', u'31646'],\n",
       " [u'2018-08-04', u'28174'],\n",
       " [u'2018-08-05', u'26989'],\n",
       " [u'2018-08-06', u'29113'],\n",
       " [u'2018-08-07', u'36983'],\n",
       " [u'2018-08-08', u'37140'],\n",
       " [u'2018-08-09', u'32216'],\n",
       " [u'2018-08-10', u'36178'],\n",
       " [u'2018-08-11', u'31293'],\n",
       " [u'2018-08-12', u'27363'],\n",
       " [u'2018-08-13', u'36076'],\n",
       " [u'2018-08-14', u'37659'],\n",
       " [u'2018-08-15', u'29270'],\n",
       " [u'2018-08-16', u'41358'],\n",
       " [u'2018-08-17', u'47634'],\n",
       " [u'2018-08-18', u'45629'],\n",
       " [u'2018-08-19', u'42281'],\n",
       " [u'2018-08-20', u'44243'],\n",
       " [u'2018-08-21', u'41964'],\n",
       " [u'2018-08-22', u'42425'],\n",
       " [u'2018-08-23', u'23616'],\n",
       " [u'2018-08-24', u'17319'],\n",
       " [u'2018-08-25', u'47109'],\n",
       " [u'2018-08-26', u'32200'],\n",
       " [u'2018-08-27', u'23382'],\n",
       " [u'2018-08-28', u'13495'],\n",
       " [u'2018-08-29', u'13001'],\n",
       " [u'2018-08-30', u'33158'],\n",
       " [u'2018-08-31', u'46101'],\n",
       " [u'2018-09-01', u'43847'],\n",
       " [u'2018-09-02', u'48427'],\n",
       " [u'2018-09-03', u'16086'],\n",
       " [u'2018-09-04', u'50500'],\n",
       " [u'2018-09-05', u'53984'],\n",
       " [u'2018-09-06', u'46532'],\n",
       " [u'2018-09-07', u'53230'],\n",
       " [u'2018-09-08', u'55470'],\n",
       " [u'2018-09-09', u'54213'],\n",
       " [u'2018-09-10', u'55188'],\n",
       " [u'2018-09-11', u'56212'],\n",
       " [u'2018-09-12', u'57363'],\n",
       " [u'2018-09-13', u'55791'],\n",
       " [u'2018-09-14', u'49383'],\n",
       " [u'2018-09-15', u'44808'],\n",
       " [u'2018-09-16', u'24317'],\n",
       " [u'2018-09-17', u'55335'],\n",
       " [u'2018-09-18', u'57691'],\n",
       " [u'2018-09-19', u'44096'],\n",
       " [u'2018-09-20', u'23616'],\n",
       " [u'2018-09-21', u'32242'],\n",
       " [u'2018-09-22', u'49688'],\n",
       " [u'2018-09-23', u'38635'],\n",
       " [u'2018-09-24', u'36406'],\n",
       " [u'2018-09-25', u'49595'],\n",
       " [u'2018-09-26', u'57730'],\n",
       " [u'2018-09-27', u'58200'],\n",
       " [u'2018-09-28', u'59649'],\n",
       " [u'2018-09-29', u'63866'],\n",
       " [u'2018-09-30', u'55893'],\n",
       " [u'2018-10-01', u'51808'],\n",
       " [u'2018-10-02', u'57280'],\n",
       " [u'2018-10-03', u'64671'],\n",
       " [u'2018-10-04', u'59413'],\n",
       " [u'2018-10-05', u'7772'],\n",
       " [u'2018-10-06', u'32510'],\n",
       " [u'2018-10-07', u'57873'],\n",
       " [u'2018-10-08', u'57247'],\n",
       " [u'2018-10-09', u'61525'],\n",
       " [u'2018-10-10', u'44876'],\n",
       " [u'2018-10-11', u'44489'],\n",
       " [u'2018-10-12', u'48529'],\n",
       " [u'2018-10-13', u'49203'],\n",
       " [u'2018-10-14', u'49025'],\n",
       " [u'2018-10-15', u'49503'],\n",
       " [u'2018-10-16', u'51236'],\n",
       " [u'2018-10-17', u'49795'],\n",
       " [u'2018-10-18', u'49560'],\n",
       " [u'2018-10-19', u'52122'],\n",
       " [u'2018-10-20', u'49815'],\n",
       " [u'2018-10-21', u'48346'],\n",
       " [u'2018-10-22', u'49889'],\n",
       " [u'2018-10-23', u'40223'],\n",
       " [u'2018-10-24', u'50343'],\n",
       " [u'2018-10-25', u'52181'],\n",
       " [u'2018-10-26', u'24308'],\n",
       " [u'2018-10-27', u'33699'],\n",
       " [u'2018-10-28', u'20882'],\n",
       " [u'2018-10-29', u'36107'],\n",
       " [u'2018-10-30', u'37382'],\n",
       " [u'2018-10-31', u'39009'],\n",
       " [u'2018-11-01', u'41911'],\n",
       " [u'2018-11-02', u'44917'],\n",
       " [u'2018-11-03', u'40640'],\n",
       " [u'2018-11-04', u'39973'],\n",
       " [u'2018-11-05', u'43870'],\n",
       " [u'2018-11-06', u'39680'],\n",
       " [u'2018-11-07', u'29492'],\n",
       " [u'2018-11-08', u'2730'],\n",
       " [u'2018-11-09', u'34673'],\n",
       " [u'2018-11-10', u'34194'],\n",
       " [u'2018-11-11', u'23926'],\n",
       " [u'2018-11-12', u'38706'],\n",
       " [u'2018-11-13', u'40263'],\n",
       " [u'2018-11-14', u'41973'],\n",
       " [u'2018-11-15', u'39702'],\n",
       " [u'2018-11-16', u'37629'],\n",
       " [u'2018-11-17', u'28167'],\n",
       " [u'2018-11-18', u'26464'],\n",
       " [u'2018-11-19', u'34352'],\n",
       " [u'2018-11-20', u'34855'],\n",
       " [u'2018-11-21', u'28140'],\n",
       " [u'2018-11-22', u'29239'],\n",
       " [u'2018-11-23', u'28620'],\n",
       " [u'2018-11-24', u'11255'],\n",
       " [u'2018-11-25', u'19946'],\n",
       " [u'2018-11-26', u'30698'],\n",
       " [u'2018-11-27', u'28806'],\n",
       " [u'2018-11-28', u'29177'],\n",
       " [u'2018-11-29', u'28977'],\n",
       " [u'2018-11-30', u'28557'],\n",
       " [u'2018-12-01', u'24622'],\n",
       " [u'2018-12-02', u'23035'],\n",
       " [u'2018-12-03', u'10638'],\n",
       " [u'2018-12-04', u'20029'],\n",
       " [u'2018-12-05', u'23535'],\n",
       " [u'2018-12-06', u'23788'],\n",
       " [u'2018-12-07', u'15148'],\n",
       " [u'2018-12-08', u'9824'],\n",
       " [u'2018-12-09', u'8749'],\n",
       " [u'2018-12-10', u'17987'],\n",
       " [u'2018-12-11', u'20805'],\n",
       " [u'2018-12-12', u'19339'],\n",
       " [u'2018-12-13', u'11765'],\n",
       " [u'2018-12-14', u'14932'],\n",
       " [u'2018-12-15', u'12720'],\n",
       " [u'2018-12-16', u'10783'],\n",
       " [u'2018-12-17', u'17449'],\n",
       " [u'2018-12-18', u'20917'],\n",
       " [u'2018-12-19', u'21970'],\n",
       " [u'2018-12-20', u'22881'],\n",
       " [u'2018-12-21', u'23422'],\n",
       " [u'2018-12-22', u'17127'],\n",
       " [u'2018-12-23', u'13601'],\n",
       " [u'2018-12-24', u'16579'],\n",
       " [u'2018-12-25', u'12437'],\n",
       " [u'2018-12-26', u'17112'],\n",
       " [u'2018-12-27', u'11325'],\n",
       " [u'2018-12-28', u'10445'],\n",
       " [u'2018-12-29', u'8083'],\n",
       " [u'2018-12-30', u'7864'],\n",
       " [u'2018-12-31', u'11911'],\n",
       " [u'2019-01-01', u'8601'],\n",
       " [u'2019-01-02', u'14360'],\n",
       " [u'2019-01-03', u'16344'],\n",
       " [u'2019-01-04', u'17308'],\n",
       " [u'2019-01-05', u'12380'],\n",
       " [u'2019-01-06', u'11094'],\n",
       " [u'2019-01-07', u'16795'],\n",
       " [u'2019-01-08', u'15833'],\n",
       " [u'2019-01-09', u'15819'],\n",
       " [u'2019-01-10', u'17894'],\n",
       " [u'2019-01-11', u'19271'],\n",
       " [u'2019-01-12', u'14519'],\n",
       " [u'2019-01-13', u'11751'],\n",
       " [u'2019-01-14', u'14818'],\n",
       " [u'2019-01-15', u'14761'],\n",
       " [u'2019-01-16', u'15823'],\n",
       " [u'2019-01-17', u'17960'],\n",
       " [u'2019-01-18', u'18901'],\n",
       " [u'2019-01-19', u'14883'],\n",
       " [u'2019-01-20', u'11191'],\n",
       " [u'2019-01-21', u'17240'],\n",
       " [u'2019-01-22', u'19885'],\n",
       " [u'2019-01-23', u'19163'],\n",
       " [u'2019-01-24', u'20057'],\n",
       " [u'2019-01-25', u'18159'],\n",
       " [u'2019-01-26', u'13203'],\n",
       " [u'2019-01-27', u'12440'],\n",
       " [u'2019-01-28', u'17317'],\n",
       " [u'2019-01-29', u'19477'],\n",
       " [u'2019-01-30', u'20677'],\n",
       " [u'2019-01-31', u'17649'],\n",
       " [u'2019-02-01', u'17696'],\n",
       " [u'2019-02-02', u'13742'],\n",
       " [u'2019-02-03', u'2010'],\n",
       " [u'2019-02-04', u'8562'],\n",
       " [u'2019-02-05', u'9119'],\n",
       " [u'2019-02-06', u'13694'],\n",
       " [u'2019-02-07', u'14533'],\n",
       " [u'2019-02-08', u'14782'],\n",
       " [u'2019-02-09', u'11760'],\n",
       " [u'2019-02-10', u'9759'],\n",
       " [u'2019-02-11', u'16881'],\n",
       " [u'2019-02-12', u'18862'],\n",
       " [u'2019-02-13', u'18411'],\n",
       " [u'2019-02-14', u'19376'],\n",
       " [u'2019-02-15', u'9213'],\n",
       " [u'2019-02-16', u'12641'],\n",
       " [u'2019-02-17', u'13390'],\n",
       " [u'2019-02-18', u'19916'],\n",
       " [u'2019-02-19', u'11689'],\n",
       " [u'2019-02-20', u'19572'],\n",
       " [u'2019-02-21', u'20995'],\n",
       " [u'2019-02-22', u'21727'],\n",
       " [u'2019-02-23', u'23742'],\n",
       " [u'2019-02-24', u'21933'],\n",
       " [u'2019-02-25', u'23445'],\n",
       " [u'2019-02-26', u'28010'],\n",
       " [u'2019-02-27', u'28652'],\n",
       " [u'2019-02-28', u'27431'],\n",
       " [u'2019-03-01', u'21751'],\n",
       " [u'2019-03-02', u'22503'],\n",
       " [u'2019-03-03', u'24382'],\n",
       " [u'2019-03-04', u'26887'],\n",
       " [u'2019-03-05', u'24042'],\n",
       " [u'2019-03-06', u'25379'],\n",
       " [u'2019-03-07', u'31156'],\n",
       " [u'2019-03-08', u'33073'],\n",
       " [u'2019-03-09', u'32421'],\n",
       " [u'2019-03-10', u'31050'],\n",
       " [u'2019-03-11', u'28539'],\n",
       " [u'2019-03-12', u'25537'],\n",
       " [u'2019-03-13', u'28653'],\n",
       " [u'2019-03-14', u'30997'],\n",
       " [u'2019-03-15', u'20481'],\n",
       " [u'2019-03-16', u'26057'],\n",
       " [u'2019-03-17', u'35622'],\n",
       " [u'2019-03-18', u'36313'],\n",
       " [u'2019-03-19', u'39908'],\n",
       " [u'2019-03-20', u'16204'],\n",
       " [u'2019-03-21', u'26455'],\n",
       " [u'2019-03-22', u'32363'],\n",
       " [u'2019-03-23', u'21696'],\n",
       " [u'2019-03-24', u'31622'],\n",
       " [u'2019-03-25', u'34226'],\n",
       " [u'2019-03-26', u'42644'],\n",
       " [u'2019-03-27', u'35832'],\n",
       " [u'2019-03-28', u'38806'],\n",
       " [u'2019-03-29', u'43893'],\n",
       " [u'2019-03-30', u'14001'],\n",
       " [u'2019-03-31', u'22326']]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bicycleRdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "bicycleDf=spark.createDataFrame(bicycleRdd, bicycleSchema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o497.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 29.0 failed 1 times, most recent failure: Lost task 0.0 in stage 29.0 (TID 226, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\Users\\G312\\Downloads\\spark-2.0.0-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 172, in main\n  File \"C:\\Users\\G312\\Downloads\\spark-2.0.0-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 167, in process\n  File \"C:\\Users\\G312\\Downloads\\spark-2.0.0-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 263, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"C:\\Users\\G312\\Downloads\\spark-2.0.0-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\sql\\session.py\", line 505, in prepare\n  File \"C:\\Users\\G312\\Downloads\\spark-2.0.0-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\sql\\types.py\", line 1349, in _verify_type\n    _verify_type(v, f.dataType, f.nullable)\n  File \"C:\\Users\\G312\\Downloads\\spark-2.0.0-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\sql\\types.py\", line 1321, in _verify_type\n    raise TypeError(\"%s can not accept object %r in type %s\" % (dataType, obj, type(obj)))\nTypeError: IntegerType can not accept object u'4950' in type <type 'unicode'>\n\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\r\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:85)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1450)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1438)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1437)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1437)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1659)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1618)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1607)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1871)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1884)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1897)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:347)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:39)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$org$apache$spark$sql$Dataset$$execute$1$1.apply(Dataset.scala:2183)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57)\r\n\tat org.apache.spark.sql.Dataset.withNewExecutionId(Dataset.scala:2532)\r\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$execute$1(Dataset.scala:2182)\r\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collect(Dataset.scala:2189)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:1925)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:1924)\r\n\tat org.apache.spark.sql.Dataset.withTypedCallback(Dataset.scala:2562)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:1924)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2139)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:239)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:280)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:128)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:211)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\Users\\G312\\Downloads\\spark-2.0.0-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 172, in main\n  File \"C:\\Users\\G312\\Downloads\\spark-2.0.0-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 167, in process\n  File \"C:\\Users\\G312\\Downloads\\spark-2.0.0-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 263, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"C:\\Users\\G312\\Downloads\\spark-2.0.0-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\sql\\session.py\", line 505, in prepare\n  File \"C:\\Users\\G312\\Downloads\\spark-2.0.0-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\sql\\types.py\", line 1349, in _verify_type\n    _verify_type(v, f.dataType, f.nullable)\n  File \"C:\\Users\\G312\\Downloads\\spark-2.0.0-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\sql\\types.py\", line 1321, in _verify_type\n    raise TypeError(\"%s can not accept object %r in type %s\" % (dataType, obj, type(obj)))\nTypeError: IntegerType can not accept object u'4950' in type <type 'unicode'>\n\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\r\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:85)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-63-034662abfb41>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mbicycleDf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\G312\\Downloads\\spark-2.0.0-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[1;34m(self, n, truncate)\u001b[0m\n\u001b[0;32m    285\u001b[0m         \u001b[1;33m+\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    286\u001b[0m         \"\"\"\n\u001b[1;32m--> 287\u001b[1;33m         \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    288\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    289\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\G312\\Downloads\\spark-2.0.0-bin-hadoop2.7\\python\\lib\\py4j-0.10.1-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    932\u001b[0m         return_value = get_return_value(\n\u001b[1;32m--> 933\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m    934\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    935\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\G312\\Downloads\\spark-2.0.0-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\G312\\Downloads\\spark-2.0.0-bin-hadoop2.7\\python\\lib\\py4j-0.10.1-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    310\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    311\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 312\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    313\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    314\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o497.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 29.0 failed 1 times, most recent failure: Lost task 0.0 in stage 29.0 (TID 226, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\Users\\G312\\Downloads\\spark-2.0.0-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 172, in main\n  File \"C:\\Users\\G312\\Downloads\\spark-2.0.0-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 167, in process\n  File \"C:\\Users\\G312\\Downloads\\spark-2.0.0-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 263, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"C:\\Users\\G312\\Downloads\\spark-2.0.0-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\sql\\session.py\", line 505, in prepare\n  File \"C:\\Users\\G312\\Downloads\\spark-2.0.0-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\sql\\types.py\", line 1349, in _verify_type\n    _verify_type(v, f.dataType, f.nullable)\n  File \"C:\\Users\\G312\\Downloads\\spark-2.0.0-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\sql\\types.py\", line 1321, in _verify_type\n    raise TypeError(\"%s can not accept object %r in type %s\" % (dataType, obj, type(obj)))\nTypeError: IntegerType can not accept object u'4950' in type <type 'unicode'>\n\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\r\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:85)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1450)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1438)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1437)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1437)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1659)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1618)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1607)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1871)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1884)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1897)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:347)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:39)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$org$apache$spark$sql$Dataset$$execute$1$1.apply(Dataset.scala:2183)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57)\r\n\tat org.apache.spark.sql.Dataset.withNewExecutionId(Dataset.scala:2532)\r\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$execute$1(Dataset.scala:2182)\r\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collect(Dataset.scala:2189)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:1925)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:1924)\r\n\tat org.apache.spark.sql.Dataset.withTypedCallback(Dataset.scala:2562)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:1924)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2139)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:239)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:280)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:128)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:211)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\Users\\G312\\Downloads\\spark-2.0.0-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 172, in main\n  File \"C:\\Users\\G312\\Downloads\\spark-2.0.0-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 167, in process\n  File \"C:\\Users\\G312\\Downloads\\spark-2.0.0-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 263, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"C:\\Users\\G312\\Downloads\\spark-2.0.0-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\sql\\session.py\", line 505, in prepare\n  File \"C:\\Users\\G312\\Downloads\\spark-2.0.0-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\sql\\types.py\", line 1349, in _verify_type\n    _verify_type(v, f.dataType, f.nullable)\n  File \"C:\\Users\\G312\\Downloads\\spark-2.0.0-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\sql\\types.py\", line 1321, in _verify_type\n    raise TypeError(\"%s can not accept object %r in type %s\" % (dataType, obj, type(obj)))\nTypeError: IntegerType can not accept object u'4950' in type <type 'unicode'>\n\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\r\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:85)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\n"
     ]
    }
   ],
   "source": [
    "bicycleDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
