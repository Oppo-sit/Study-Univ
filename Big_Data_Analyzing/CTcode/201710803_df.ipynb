{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ[\"PYLIB\"]=os.path.join(os.environ[\"SPARK_HOME\"],'python','lib')\n",
    "sys.path.insert(0,os.path.join(os.environ[\"PYLIB\"],'py4j-0.10.1-src.zip'))\n",
    "sys.path.insert(0,os.path.join(os.environ[\"PYLIB\"],'pyspark.zip'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "myConf=pyspark.SparkConf() # SparkSession에 필요한 설정을 삽입해서 만듦. 지금은 공란\n",
    "spark = pyspark.sql.SparkSession.builder\\\n",
    "    .master(\"local\")\\\n",
    "    .appName(\"myApp\")\\\n",
    "    .config(\"spark.sql.warehouse.dir\", \"C:\\users\\g312\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "print spark.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "myList=[('1','kim, js',170),\n",
    "        ('1','lee, sm', 175),\n",
    "        ('2','lim, yg',180),\n",
    "        ('2','lee',170)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "myDf=spark.createDataFrame(myList) #내부든 외부든 createDataFrame쓰면 DF가 생성됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _1: string (nullable = true)\n",
      " |-- _2: string (nullable = true)\n",
      " |-- _3: long (nullable = true)\n",
      "\n",
      "[Row(_1=u'1', _2=u'kim, js', _3=170)]\n"
     ]
    }
   ],
   "source": [
    "myDf.printSchema() #앞 두 column은 string ('')으로, 마지막 column은 long으로 자동 지정\n",
    "print myDf.take(1) #column이름이 없으므로 열은 _1, _2와 같이 표시됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(year=u'1', name=u'kim, js', height=170)]\n"
     ]
    }
   ],
   "source": [
    "print spark.createDataFrame(myList, ['year','name','height']).take(1)\n",
    "#이와 같이 열의 이름을 설정하면 출력시에도 열의 이름으로 보여준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+\n",
      "|name|      item|\n",
      "+----+----------+\n",
      "| kim|  espresso|\n",
      "| lee|     latte|\n",
      "| lee| americano|\n",
      "| lim|  affocato|\n",
      "| kim|long black|\n",
      "| lee|  macciato|\n",
      "| lee|  espresso|\n",
      "| lim|     latte|\n",
      "| kim| americano|\n",
      "| lee|  affocato|\n",
      "+----+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "names = [\"kim\",\"lee\",\"lee\",\"lim\"]\n",
    "items = [\"espresso\",\"latte\",\"americano\",\"affocato\",\"long black\",\"macciato\"]\n",
    "df = spark.createDataFrame([(names[i%4], items[i%6]) for i in range(100)],\\\n",
    "                           [\"name\",\"item\"])\n",
    "#위에 있는 이름과 아이템들을 순서대로 100번 반복반환한다.\n",
    "#names[i%4], items[i%6] i->0, 1, 2, 3 and i->0, 1, 2, 3, 4, 5\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|short name|\n",
      "+----------+\n",
      "|       esp|\n",
      "|       lat|\n",
      "|       ame|\n",
      "+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.item.substr(1, 3).alias(\"short name\")).show(3)\n",
    "#substr -> 선택할 문자의 range(현재는 세글자), alias -> 컬럼의 이름"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row # Row 객체를 사용한다.\n",
    "Person = Row('year','name', 'height') #Row안(속성)에는 column의 이름이 들어감.\n",
    "row1=Person('1','kim, js',170)#Person이란 Row 객체에 맞춰 행을 입력."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row1:  1 kim, js\n"
     ]
    }
   ],
   "source": [
    "print \"row1: \", row1.year, row1.name #row.key 방식으로 출력, row[key]로도 출력된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "myRows = [row1, #myRows(DataFrame)에 들어갈 객체로 만들어진 객체를 넣을 수 있다.\n",
    "          Person('1','lee, sm', 175),\n",
    "          Person('2','lim, yg',180),\n",
    "          Person('2','lee',170)] \n",
    "#DataFrame을 List의 형태로 생성하는 방식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "myDf=spark.createDataFrame(myRows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- year: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- height: long (nullable = true)\n",
      "\n",
      "None\n",
      "+----+-------+------+\n",
      "|year|   name|height|\n",
      "+----+-------+------+\n",
      "|   1|kim, js|   170|\n",
      "|   1|lee, sm|   175|\n",
      "|   2|lim, yg|   180|\n",
      "|   2|    lee|   170|\n",
      "+----+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print myDf.printSchema()\n",
    "myDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Schema를 정의해보자.\n",
    "from pyspark.sql.types import StructType, StructField\n",
    "from pyspark.sql.types import StringType, IntegerType\n",
    "mySchema=StructType([\n",
    "    StructField(\"year\", StringType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"height\", IntegerType(), True)\n",
    "])\n",
    "#현재 StructField엔 column의 명칭, 타입, 그리고 Null의 허용여부가 순서대로 입력되어 있다\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- year: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- height: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(year=u'1', name=u'kim, js', height=170)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myDf=spark.createDataFrame(myRows, mySchema) #처음과 다르게 리스트와 스키마를 같이 부름.\n",
    "myDf.printSchema() #결과에서 보이듯이 자동지정된 Long이 아닌 Int형이다.\n",
    "myDf.take(1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rdd로 DF를 생성해보자.\n",
    "from pyspark.sql import Row\n",
    "\n",
    "myList=[('1','kim, js',170),('1','lee, sm', 175),('2','lim, yg',180),('2','lee',170)]\n",
    "myRdd = spark.sparkContext.parallelize(myList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _1: string (nullable = true)\n",
      " |-- _2: string (nullable = true)\n",
      " |-- _3: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rddDf=myRdd.toDF() #toDF 함수로 RDD가 DF로 형변환된다. Schema가 없으므로 자동지정됨.\n",
    "rddDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _1: string (nullable = true)\n",
      " |-- _2: string (nullable = true)\n",
      " |-- _3: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rddDf=spark.createDataFrame(myRdd) #RDD를 생성함수로도 변환시킬 수 있다.\n",
    "rddDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "| _1|     _2|\n",
      "+---+-------+\n",
      "|  1|kim, js|\n",
      "|  2|    lee|\n",
      "+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rddDf.where(rddDf._3 < 175)\\\n",
    "    .select([rddDf._1, rddDf._2])\\\n",
    "    .show() \n",
    "#데이터에서 _3(세번째 column)의 value<175인 row는?\n",
    "#1, 2번 column을 선택\n",
    "#선택된 column과 row만 보여준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "| _1|max(_3)|\n",
      "+---+-------+\n",
      "|  1|    175|\n",
      "|  2|    180|\n",
      "+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rddDf.groupby(rddDf._1).max().show()\n",
    "#이거 어려워 보이는데 쉬움.\n",
    "#일단 DF를 rddDf._1(key)로 그룹핑하고 그룹마다의 value의 최댓값을 골라 이를 출력한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Row 객체로 위를 구현해보자.\n",
    "_myRdd=myRdd.map(lambda x:Row(year=int(x[0]),name=x[1],height=int(x[2])))\n",
    "#year는 int로 자동 형변환된다.\n",
    "#맵에서 column이름을 정해준다.\n",
    "#RDD에서도 Row 객체를 쓸 수 있구나."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- height: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- year: long (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(height=170, name=u'kim, js', year=1)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_myDf=spark.createDataFrame(_myRdd) # 위와 같은 과정\n",
    "_myDf.printSchema() #위에선 int로 받았지만, schema 설정한 것은 아니므로 자동지정\n",
    "_myDf.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import StructType, StructField\n",
    "from pyspark.sql.types import StringType, IntegerType, TimestampType\n",
    "r1=Row(name=\"js1\",age=10)\n",
    "r2=Row(name=\"js2\",age=20)\n",
    "_myRdd=spark.sparkContext.parallelize([r1,r2])\n",
    "#Row 객체로 RDD를 생성할 수 있는 것이었다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(age=10, name='js1'), Row(age=20, name='js2')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_myRdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n",
      "+---+----+\n",
      "|age|name|\n",
      "+---+----+\n",
      "| 10| js1|\n",
      "| 20| js2|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#createDataFrame함수에 (Rdd, schema)로 정의한 schema를 생성할 수 있다.\n",
    "schema=StructType([\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    #StructField(\"created\", TimestampType(), True)\n",
    "])\n",
    "_myDf=spark.createDataFrame(_myRdd,schema)\n",
    "_myDf.printSchema()\n",
    "_myDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- height: double (nullable = true)\n",
      "\n",
      "+---+----+------+\n",
      "| id|name|height|\n",
      "+---+----+------+\n",
      "|  1| kim|  50.0|\n",
      "|  2| lee|  60.0|\n",
      "|  3|park|  70.0|\n",
      "+---+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#위의 RDD->DF + Schema를 정리하면 아래와 같다.\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "myRdd=spark.sparkContext.parallelize([(1, 'kim', 50.0), (2, 'lee', 60.0), (3, 'park', 70.0)])\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"height\", DoubleType(), True)\n",
    "])\n",
    "_myDf = spark.createDataFrame(myRdd, schema)\n",
    "_myDf.printSchema()\n",
    "_myDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>name</th>\n",
       "      <th>height</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>kim, js</td>\n",
       "      <td>170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>lee, sm</td>\n",
       "      <td>175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>lim, yg</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>lee</td>\n",
       "      <td>170</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  year     name  height\n",
       "0    1  kim, js     170\n",
       "1    1  lee, sm     175\n",
       "2    2  lim, yg     180\n",
       "3    2      lee     170"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myDf.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#myDf.write.format('com.databricks.spark.csv').save(os.path.join('data','_myDf.csv')) #error 발생."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!doskey ls = dir #doskey ls란 이름으로 dir 기능을 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting data\\ds_twitter_seoul_3.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile data\\ds_twitter_seoul_3.json\n",
    "{\"contributors\": null, \"truncated\": false, \"text\": \"RT @soompi: #SEVENTEEN’s Mingyu, Jin Se Yeon, And Leeteuk To MC For 2016 Super Seoul Dream Concert \\nhttps://t.co/1XRSaRBbE0 https://t.co/fi…\", \"is_quote_status\": false, \"in_reply_to_status_id\": null, \"id\": 801657325836763136, \"favorite_count\": 0, \"entities\": {\"symbols\": [], \"user_mentions\": [{\"id\": 17659206, \"indices\": [3, 10], \"id_str\": \"17659206\", \"screen_name\": \"soompi\", \"name\": \"Soompi\"}], \"hashtags\": [{\"indices\": [12, 22], \"text\": \"SEVENTEEN\"}], \"urls\": [{\"url\": \"https://t.co/1XRSaRBbE0\", \"indices\": [100, 123], \"expanded_url\": \"http://www.soompi.com/2016/11/20/seventeens-mingyu-jin-se-yeon-leeteuk-mc-dream-concert/\", \"display_url\": \"soompi.com/2016/11/20/sev…\"}]}, \"retweeted\": false, \"coordinates\": null, \"source\": \"<a href=\\\"http://twitter.com/download/android\\\" rel=\\\"nofollow\\\">Twitter for Android</a>\", \"in_reply_to_screen_name\": null, \"in_reply_to_user_id\": null, \"retweet_count\": 1487, \"id_str\": \"801657325836763136\", \"favorited\": false, \"retweeted_status\": {\"contributors\": null, \"truncated\": false, \"text\": \"#SEVENTEEN’s Mingyu, Jin Se Yeon, And Leeteuk To MC For 2016 Super Seoul Dream Concert \\nhttps://t.co/1XRSaRBbE0 https://t.co/fifXHpF8or\", \"is_quote_status\": false, \"in_reply_to_status_id\": null, \"id\": 800593781586132993, \"favorite_count\": 1649, \"entities\": {\"symbols\": [], \"user_mentions\": [], \"hashtags\": [{\"indices\": [0, 10], \"text\": \"SEVENTEEN\"}], \"urls\": [{\"url\": \"https://t.co/1XRSaRBbE0\", \"indices\": [88, 111], \"expanded_url\": \"http://www.soompi.com/2016/11/20/seventeens-mingyu-jin-se-yeon-leeteuk-mc-dream-concert/\", \"display_url\": \"soompi.com/2016/11/20/sev…\"}], \"media\": [{\"expanded_url\": \"https://twitter.com/soompi/status/800593781586132993/photo/1\", \"display_url\": \"pic.twitter.com/fifXHpF8or\", \"url\": \"https://t.co/fifXHpF8or\", \"media_url_https\": \"https://pbs.twimg.com/media/CxxHMk8UsAA4cUT.jpg\", \"id_str\": \"800593115165798400\", \"sizes\": {\"small\": {\"h\": 382, \"resize\": \"fit\", \"w\": 680}, \"large\": {\"h\": 449, \"resize\": \"fit\", \"w\": 800}, \"medium\": {\"h\": 449, \"resize\": \"fit\", \"w\": 800}, \"thumb\": {\"h\": 150, \"resize\": \"crop\", \"w\": 150}}, \"indices\": [112, 135], \"type\": \"photo\", \"id\": 800593115165798400, \"media_url\": \"http://pbs.twimg.com/media/CxxHMk8UsAA4cUT.jpg\"}]}, \"retweeted\": false, \"coordinates\": null, \"source\": \"<a href=\\\"https://about.twitter.com/products/tweetdeck\\\" rel=\\\"nofollow\\\">TweetDeck</a>\", \"in_reply_to_screen_name\": null, \"in_reply_to_user_id\": null, \"retweet_count\": 1487, \"id_str\": \"800593781586132993\", \"favorited\": false, \"user\": {\"follow_request_sent\": false, \"has_extended_profile\": true, \"profile_use_background_image\": true, \"default_profile_image\": false, \"id\": 17659206, \"profile_background_image_url_https\": \"https://pbs.twimg.com/profile_background_images/699864769/1cdde0a85f5c0a994ae1fb06d545a5ec.png\", \"verified\": true, \"translator_type\": \"none\", \"profile_text_color\": \"999999\", \"profile_image_url_https\": \"https://pbs.twimg.com/profile_images/792117259489583104/4khJk3zz_normal.jpg\", \"profile_sidebar_fill_color\": \"000000\", \"entities\": {\"url\": {\"urls\": [{\"url\": \"http://t.co/3evT80UlR9\", \"indices\": [0, 22], \"expanded_url\": \"http://www.soompi.com\", \"display_url\": \"soompi.com\"}]}, \"description\": {\"urls\": []}}, \"followers_count\": 987867, \"profile_sidebar_border_color\": \"000000\", \"id_str\": \"17659206\", \"profile_background_color\": \"1E1E1E\", \"listed_count\": 3982, \"is_translation_enabled\": true, \"utc_offset\": -28800, \"statuses_count\": 80038, \"description\": \"The original K-pop community. We take gifs, OTPs, and reporting on your bias' fashion choices seriously. But not rumors. Ain't nobody got time for that.\", \"friends_count\": 3532, \"location\": \"Worldwide\", \"profile_link_color\": \"31B6F4\", \"profile_image_url\": \"http://pbs.twimg.com/profile_images/792117259489583104/4khJk3zz_normal.jpg\", \"following\": false, \"geo_enabled\": false, \"profile_banner_url\": \"https://pbs.twimg.com/profile_banners/17659206/1478803767\", \"profile_background_image_url\": \"http://pbs.twimg.com/profile_background_images/699864769/1cdde0a85f5c0a994ae1fb06d545a5ec.png\", \"screen_name\": \"soompi\", \"lang\": \"en\", \"profile_background_tile\": true, \"favourites_count\": 1493, \"name\": \"Soompi\", \"notifications\": false, \"url\": \"http://t.co/3evT80UlR9\", \"created_at\": \"Wed Nov 26 20:48:27 +0000 2008\", \"contributors_enabled\": false, \"time_zone\": \"Pacific Time (US & Canada)\", \"protected\": false, \"default_profile\": false, \"is_translator\": false}, \"geo\": null, \"in_reply_to_user_id_str\": null, \"possibly_sensitive\": false, \"lang\": \"en\", \"created_at\": \"Mon Nov 21 06:56:46 +0000 2016\", \"in_reply_to_status_id_str\": null, \"place\": null, \"extended_entities\": {\"media\": [{\"expanded_url\": \"https://twitter.com/soompi/status/800593781586132993/photo/1\", \"display_url\": \"pic.twitter.com/fifXHpF8or\", \"url\": \"https://t.co/fifXHpF8or\", \"media_url_https\": \"https://pbs.twimg.com/media/CxxHMk8UsAA4cUT.jpg\", \"id_str\": \"800593115165798400\", \"sizes\": {\"small\": {\"h\": 382, \"resize\": \"fit\", \"w\": 680}, \"large\": {\"h\": 449, \"resize\": \"fit\", \"w\": 800}, \"medium\": {\"h\": 449, \"resize\": \"fit\", \"w\": 800}, \"thumb\": {\"h\": 150, \"resize\": \"crop\", \"w\": 150}}, \"indices\": [112, 135], \"type\": \"photo\", \"id\": 800593115165798400, \"media_url\": \"http://pbs.twimg.com/media/CxxHMk8UsAA4cUT.jpg\"}]}, \"metadata\": {\"iso_language_code\": \"en\", \"result_type\": \"recent\"}}, \"user\": {\"follow_request_sent\": false, \"has_extended_profile\": false, \"profile_use_background_image\": true, \"default_profile_image\": true, \"id\": 791090169818521600, \"profile_background_image_url_https\": null, \"verified\": false, \"translator_type\": \"none\", \"profile_text_color\": \"333333\", \"profile_image_url_https\": \"https://abs.twimg.com/sticky/default_profile_images/default_profile_6_normal.png\", \"profile_sidebar_fill_color\": \"DDEEF6\", \"entities\": {\"description\": {\"urls\": []}}, \"followers_count\": 0, \"profile_sidebar_border_color\": \"C0DEED\", \"id_str\": \"791090169818521600\", \"profile_background_color\": \"F5F8FA\", \"listed_count\": 0, \"is_translation_enabled\": false, \"utc_offset\": null, \"statuses_count\": 96, \"description\": \"\", \"friends_count\": 7, \"location\": \"\", \"profile_link_color\": \"1DA1F2\", \"profile_image_url\": \"http://abs.twimg.com/sticky/default_profile_images/default_profile_6_normal.png\", \"following\": false, \"geo_enabled\": false, \"profile_background_image_url\": null, \"screen_name\": \"enriquesanq\", \"lang\": \"es\", \"profile_background_tile\": false, \"favourites_count\": 161, \"name\": \"Enrique santos\", \"notifications\": false, \"url\": null, \"created_at\": \"Wed Oct 26 01:32:49 +0000 2016\", \"contributors_enabled\": false, \"time_zone\": null, \"protected\": false, \"default_profile\": true, \"is_translator\": false}, \"geo\": null, \"in_reply_to_user_id_str\": null, \"possibly_sensitive\": false, \"lang\": \"en\", \"created_at\": \"Thu Nov 24 05:22:55 +0000 2016\", \"in_reply_to_status_id_str\": null, \"place\": null, \"metadata\": {\"iso_language_code\": \"en\", \"result_type\": \"recent\"}}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['{\"contributors\": null, \"truncated\": false, \"text\": \"RT @soompi: #SEVENTEEN\\xe2\\x80\\x99s Mingyu, Jin Se Yeon, And Leeteuk To MC For 2016 Super Seoul Dream Concert \\\\nhttps://t.co/1XRSaRBbE0 https://t.co/fi\\xe2\\x80\\xa6\", \"is_quote_status\": false, \"in_reply_to_status_id\": null, \"id\": 801657325836763136, \"favorite_count\": 0, \"entities\": {\"symbols\": [], \"user_mentions\": [{\"id\": 17659206, \"indices\": [3, 10], \"id_str\": \"17659206\", \"screen_name\": \"soompi\", \"name\": \"Soompi\"}], \"hashtags\": [{\"indices\": [12, 22], \"text\": \"SEVENTEEN\"}], \"urls\": [{\"url\": \"https://t.co/1XRSaRBbE0\", \"indices\": [100, 123], \"expanded_url\": \"http://www.soompi.com/2016/11/20/seventeens-mingyu-jin-se-yeon-leeteuk-mc-dream-concert/\", \"display_url\": \"soompi.com/2016/11/20/sev\\xe2\\x80\\xa6\"}]}, \"retweeted\": false, \"coordinates\": null, \"source\": \"<a href=\\\\\"http://twitter.com/download/android\\\\\" rel=\\\\\"nofollow\\\\\">Twitter for Android</a>\", \"in_reply_to_screen_name\": null, \"in_reply_to_user_id\": null, \"retweet_count\": 1487, \"id_str\": \"801657325836763136\", \"favorited\": false, \"retweeted_status\": {\"contributors\": null, \"truncated\": false, \"text\": \"#SEVENTEEN\\xe2\\x80\\x99s Mingyu, Jin Se Yeon, And Leeteuk To MC For 2016 Super Seoul Dream Concert \\\\nhttps://t.co/1XRSaRBbE0 https://t.co/fifXHpF8or\", \"is_quote_status\": false, \"in_reply_to_status_id\": null, \"id\": 800593781586132993, \"favorite_count\": 1649, \"entities\": {\"symbols\": [], \"user_mentions\": [], \"hashtags\": [{\"indices\": [0, 10], \"text\": \"SEVENTEEN\"}], \"urls\": [{\"url\": \"https://t.co/1XRSaRBbE0\", \"indices\": [88, 111], \"expanded_url\": \"http://www.soompi.com/2016/11/20/seventeens-mingyu-jin-se-yeon-leeteuk-mc-dream-concert/\", \"display_url\": \"soompi.com/2016/11/20/sev\\xe2\\x80\\xa6\"}], \"media\": [{\"expanded_url\": \"https://twitter.com/soompi/status/800593781586132993/photo/1\", \"display_url\": \"pic.twitter.com/fifXHpF8or\", \"url\": \"https://t.co/fifXHpF8or\", \"media_url_https\": \"https://pbs.twimg.com/media/CxxHMk8UsAA4cUT.jpg\", \"id_str\": \"800593115165798400\", \"sizes\": {\"small\": {\"h\": 382, \"resize\": \"fit\", \"w\": 680}, \"large\": {\"h\": 449, \"resize\": \"fit\", \"w\": 800}, \"medium\": {\"h\": 449, \"resize\": \"fit\", \"w\": 800}, \"thumb\": {\"h\": 150, \"resize\": \"crop\", \"w\": 150}}, \"indices\": [112, 135], \"type\": \"photo\", \"id\": 800593115165798400, \"media_url\": \"http://pbs.twimg.com/media/CxxHMk8UsAA4cUT.jpg\"}]}, \"retweeted\": false, \"coordinates\": null, \"source\": \"<a href=\\\\\"https://about.twitter.com/products/tweetdeck\\\\\" rel=\\\\\"nofollow\\\\\">TweetDeck</a>\", \"in_reply_to_screen_name\": null, \"in_reply_to_user_id\": null, \"retweet_count\": 1487, \"id_str\": \"800593781586132993\", \"favorited\": false, \"user\": {\"follow_request_sent\": false, \"has_extended_profile\": true, \"profile_use_background_image\": true, \"default_profile_image\": false, \"id\": 17659206, \"profile_background_image_url_https\": \"https://pbs.twimg.com/profile_background_images/699864769/1cdde0a85f5c0a994ae1fb06d545a5ec.png\", \"verified\": true, \"translator_type\": \"none\", \"profile_text_color\": \"999999\", \"profile_image_url_https\": \"https://pbs.twimg.com/profile_images/792117259489583104/4khJk3zz_normal.jpg\", \"profile_sidebar_fill_color\": \"000000\", \"entities\": {\"url\": {\"urls\": [{\"url\": \"http://t.co/3evT80UlR9\", \"indices\": [0, 22], \"expanded_url\": \"http://www.soompi.com\", \"display_url\": \"soompi.com\"}]}, \"description\": {\"urls\": []}}, \"followers_count\": 987867, \"profile_sidebar_border_color\": \"000000\", \"id_str\": \"17659206\", \"profile_background_color\": \"1E1E1E\", \"listed_count\": 3982, \"is_translation_enabled\": true, \"utc_offset\": -28800, \"statuses_count\": 80038, \"description\": \"The original K-pop community. We take gifs, OTPs, and reporting on your bias\\' fashion choices seriously. But not rumors. Ain\\'t nobody got time for that.\", \"friends_count\": 3532, \"location\": \"Worldwide\", \"profile_link_color\": \"31B6F4\", \"profile_image_url\": \"http://pbs.twimg.com/profile_images/792117259489583104/4khJk3zz_normal.jpg\", \"following\": false, \"geo_enabled\": false, \"profile_banner_url\": \"https://pbs.twimg.com/profile_banners/17659206/1478803767\", \"profile_background_image_url\": \"http://pbs.twimg.com/profile_background_images/699864769/1cdde0a85f5c0a994ae1fb06d545a5ec.png\", \"screen_name\": \"soompi\", \"lang\": \"en\", \"profile_background_tile\": true, \"favourites_count\": 1493, \"name\": \"Soompi\", \"notifications\": false, \"url\": \"http://t.co/3evT80UlR9\", \"created_at\": \"Wed Nov 26 20:48:27 +0000 2008\", \"contributors_enabled\": false, \"time_zone\": \"Pacific Time (US & Canada)\", \"protected\": false, \"default_profile\": false, \"is_translator\": false}, \"geo\": null, \"in_reply_to_user_id_str\": null, \"possibly_sensitive\": false, \"lang\": \"en\", \"created_at\": \"Mon Nov 21 06:56:46 +0000 2016\", \"in_reply_to_status_id_str\": null, \"place\": null, \"extended_entities\": {\"media\": [{\"expanded_url\": \"https://twitter.com/soompi/status/800593781586132993/photo/1\", \"display_url\": \"pic.twitter.com/fifXHpF8or\", \"url\": \"https://t.co/fifXHpF8or\", \"media_url_https\": \"https://pbs.twimg.com/media/CxxHMk8UsAA4cUT.jpg\", \"id_str\": \"800593115165798400\", \"sizes\": {\"small\": {\"h\": 382, \"resize\": \"fit\", \"w\": 680}, \"large\": {\"h\": 449, \"resize\": \"fit\", \"w\": 800}, \"medium\": {\"h\": 449, \"resize\": \"fit\", \"w\": 800}, \"thumb\": {\"h\": 150, \"resize\": \"crop\", \"w\": 150}}, \"indices\": [112, 135], \"type\": \"photo\", \"id\": 800593115165798400, \"media_url\": \"http://pbs.twimg.com/media/CxxHMk8UsAA4cUT.jpg\"}]}, \"metadata\": {\"iso_language_code\": \"en\", \"result_type\": \"recent\"}}, \"user\": {\"follow_request_sent\": false, \"has_extended_profile\": false, \"profile_use_background_image\": true, \"default_profile_image\": true, \"id\": 791090169818521600, \"profile_background_image_url_https\": null, \"verified\": false, \"translator_type\": \"none\", \"profile_text_color\": \"333333\", \"profile_image_url_https\": \"https://abs.twimg.com/sticky/default_profile_images/default_profile_6_normal.png\", \"profile_sidebar_fill_color\": \"DDEEF6\", \"entities\": {\"description\": {\"urls\": []}}, \"followers_count\": 0, \"profile_sidebar_border_color\": \"C0DEED\", \"id_str\": \"791090169818521600\", \"profile_background_color\": \"F5F8FA\", \"listed_count\": 0, \"is_translation_enabled\": false, \"utc_offset\": null, \"statuses_count\": 96, \"description\": \"\", \"friends_count\": 7, \"location\": \"\", \"profile_link_color\": \"1DA1F2\", \"profile_image_url\": \"http://abs.twimg.com/sticky/default_profile_images/default_profile_6_normal.png\", \"following\": false, \"geo_enabled\": false, \"profile_background_image_url\": null, \"screen_name\": \"enriquesanq\", \"lang\": \"es\", \"profile_background_tile\": false, \"favourites_count\": 161, \"name\": \"Enrique santos\", \"notifications\": false, \"url\": null, \"created_at\": \"Wed Oct 26 01:32:49 +0000 2016\", \"contributors_enabled\": false, \"time_zone\": null, \"protected\": false, \"default_profile\": true, \"is_translator\": false}, \"geo\": null, \"in_reply_to_user_id_str\": null, \"possibly_sensitive\": false, \"lang\": \"en\", \"created_at\": \"Thu Nov 24 05:22:55 +0000 2016\", \"in_reply_to_status_id_str\": null, \"place\": null, \"metadata\": {\"iso_language_code\": \"en\", \"result_type\": \"recent\"}}']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "_jfname=os.path.join('data','ds_twitter_seoul_3.json')\n",
    "with open(_jfname, 'rb') as f:\n",
    "    data = f.readlines()\n",
    "    \n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = map(lambda x: x.rstrip(), data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A+B+C\n"
     ]
    }
   ],
   "source": [
    "print \"+\".join([\"A\", \"B\", \"C\"]) #join 함수는 앞의 문자와 안의 인자를 결합\n",
    "\n",
    "data_json_str = \"[\" + ','.join(data) + \"]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6908"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_json_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_df = pd.read_json(data_json_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "contributors                 0\n",
      "coordinates                  0\n",
      "created_at                   1\n",
      "entities                     1\n",
      "favorite_count               1\n",
      "favorited                    1\n",
      "geo                          0\n",
      "id                           1\n",
      "id_str                       1\n",
      "in_reply_to_screen_name      0\n",
      "in_reply_to_status_id        0\n",
      "in_reply_to_status_id_str    0\n",
      "in_reply_to_user_id          0\n",
      "in_reply_to_user_id_str      0\n",
      "is_quote_status              1\n",
      "lang                         1\n",
      "metadata                     1\n",
      "place                        0\n",
      "possibly_sensitive           1\n",
      "retweet_count                1\n",
      "retweeted                    1\n",
      "retweeted_status             1\n",
      "source                       1\n",
      "text                         1\n",
      "truncated                    1\n",
      "user                         1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print data_df.count() #행의 개수  #전자는 column명이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    801657325836763136\n",
       "Name: id, dtype: int64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df['id'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function numpy.array>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _1: string (nullable = true)\n",
      " |-- _2: string (nullable = true)\n",
      " |-- _3: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(_1=u'1', _2=u'65.78', _3=u'112.99')]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "rdd=spark.sparkContext\\\n",
    "    .textFile(os.path.join('data','ds_spark_heightweight.txt'))\n",
    "\n",
    "tRdd=rdd.map(lambda x:x.split('\\t'))\n",
    "tDf=spark.createDataFrame(tRdd)\n",
    "\n",
    "tDf.printSchema()\n",
    "\n",
    "tDf.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=1, height=65.78, weight=112.99)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tDf=tDf.withColumn(\"id\",tDf['_1'].cast(\"integer\")).drop('_1')\n",
    "tDf=tDf.withColumn(\"height\",tDf['_2'].cast(\"double\")).drop('_2')\n",
    "tDf=tDf.withColumn(\"weight\",tDf['_3'].cast(\"double\")).drop('_3')\n",
    "\n",
    "tDf.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: double (nullable = true)\n",
      " |-- weight: double (nullable = true)\n",
      " |-- height: double (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(id=1.0, weight=65.78, height=112.99)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tRdd=rdd.map(lambda line:[float(x) for x in line.split('\\t')])\n",
    "tRdd.take(1)\n",
    "\n",
    "tDf=spark.createDataFrame(tRdd,[\"id\",\"weight\",\"height\"])\n",
    "\n",
    "tDf.printSchema()\n",
    "tDf.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[65.78 71.52 69.4  68.22 67.79]\n",
      "[112.99 136.49 153.03 142.34 144.3 ]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "_weightRdd=tDf.rdd.map(lambda fields:fields[1]).collect()\n",
    "_heightRdd=tDf.rdd.map(lambda fields:fields[2]).collect()\n",
    "print np.array(_weightRdd)[:5]\n",
    "print np.array(_heightRdd)[:5]\n",
    "\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(np.array(_weightRdd), np.array(_heightRdd),'o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#년별 분기별 대여건수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "_bicycle = spark.read.format('com.databricks.spark.csv')\\\n",
    "    .options(header='true', inferschema='true').load('data/bicycle.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: timestamp (nullable = true)\n",
      " |-- count: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_bicycle.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "455"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_bicycle.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|                date|count|\n",
      "+--------------------+-----+\n",
      "|2018-01-01 00:00:...| 4950|\n",
      "|2018-01-02 00:00:...| 7136|\n",
      "|2018-01-03 00:00:...| 7156|\n",
      "|2018-01-04 00:00:...| 7102|\n",
      "|2018-01-05 00:00:...| 7705|\n",
      "+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_bicycle.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "bicycle=_bicycle\\\n",
    "    .withColumnRenamed(\"date\", \"Date\")\\\n",
    "    .withColumnRenamed(\" count\", \"Count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|                Date|count|\n",
      "+--------------------+-----+\n",
      "|2018-01-01 00:00:...| 4950|\n",
      "|2018-01-02 00:00:...| 7136|\n",
      "|2018-01-03 00:00:...| 7156|\n",
      "|2018-01-04 00:00:...| 7102|\n",
      "|2018-01-05 00:00:...| 7705|\n",
      "+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bicycle.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "bicycle=bicycle.withColumn(\"year\",bicycle.Date.substr(1, 4))\n",
    "bicycle=bicycle.withColumn(\"month\",bicycle.Date.substr(6, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "bicycle = bicycle\\\n",
    "    .withColumn('year', F.year('date'))\\\n",
    "    .withColumn('month', F.month('date'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+\n",
      "|year|sum(count)|\n",
      "+----+----------+\n",
      "|2018|  10124874|\n",
      "|2019|   1871935|\n",
      "+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bicycle.groupBy('year').agg({\"count\":\"sum\"}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+------+------+------+------+-------+-------+-------+-------+-------+------+------+\n",
      "|year|     1|     2|     3|     4|     5|      6|      7|      8|      9|     10|    11|    12|\n",
      "+----+------+------+------+------+------+-------+-------+-------+-------+-------+------+------+\n",
      "|2018|164367|168741|462661|687885|965609|1207123|1100015|1037505|1447993|1420621|961532|500822|\n",
      "|2019|495573|471543|904819|  null|  null|   null|   null|   null|   null|   null|  null|  null|\n",
      "+----+------+------+------+------+------+-------+-------+-------+-------+-------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bicycle.groupBy('year').pivot('month').agg({\"count\":\"sum\"}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Json 파일 읽기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#C:\\Users\\G312\\Downloads\\spark-2.0.0-bin-hadoop2.7\\examples\\src\\main\\resources에서 people.json을 추출\n",
    "jfile= os.path.join(os.environ[\"SPARK_HOME\"],\\\n",
    "           \"examples/src/main/resources/people.json\")\n",
    "\n",
    "_myDf= spark.read.json(jfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "|age|name|\n",
      "+---+----+\n",
      "| 30|Andy|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_myDf.filter(_myDf['age'] > 21).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "r=requests.get(\"https://raw.githubusercontent.com/jokecamp/FootballData/master/World%20Cups/all-world-cup-players.json\")\n",
    "wc=r.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'list'> <type 'dict'>\n"
     ]
    }
   ],
   "source": [
    "print type(wc), type(wc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'Club': u'Club Atl\\xc3\\xa9tico Talleres de Remedios de Escalada',\n",
       " u'ClubCountry': u'Argentina',\n",
       " u'Competition': u'World Cup',\n",
       " u'DateOfBirth': u'1905-5-5',\n",
       " u'FullName': u'\\xc3ngel Bossio',\n",
       " u'IsCaptain': False,\n",
       " u'Number': u'',\n",
       " u'Position': u'GK',\n",
       " u'Team': u'Argentina',\n",
       " u'Year': 1930}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wc[0] # dictionary의 리스트로 구성되어 있어 dictionary(중괄호)가 하나의 단위"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Club: string (nullable = true)\n",
      " |-- ClubCountry: string (nullable = true)\n",
      " |-- Competition: string (nullable = true)\n",
      " |-- DateOfBirth: string (nullable = true)\n",
      " |-- FullName: string (nullable = true)\n",
      " |-- IsCaptain: boolean (nullable = true)\n",
      " |-- Number: string (nullable = true)\n",
      " |-- Position: string (nullable = true)\n",
      " |-- Team: string (nullable = true)\n",
      " |-- Year: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## 웹데이터는 항상 '문자' 취급 해야 한다.\n",
    "## 또한 데이터를 수집하고 불러올 때 '통계적'으로 생각해보자\n",
    "## 국가별로 몇 명있지? 성씨 같은 사람이 몇 명이나 있지?\n",
    "wcDF=spark.createDataFrame(wc)\n",
    "wcDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Club=u'Club Atl\\xc3\\xa9tico Talleres de Remedios de Escalada', ClubCountry=u'Argentina', Competition=u'World Cup', DateOfBirth=u'1905-5-5', FullName=u'\\xc3ngel Bossio', IsCaptain=False, Number=u'', Position=u'GK', Team=u'Argentina', Year=1930)]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wcDF.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1991-11-25 00:00:00\n"
     ]
    }
   ],
   "source": [
    "#dateType 형변환\n",
    "from datetime import datetime\n",
    "print datetime.strptime(\"11/25/1991\", '%m/%d/%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Club: string (nullable = true)\n",
      " |-- ClubCountry: string (nullable = true)\n",
      " |-- Competition: string (nullable = true)\n",
      " |-- DateOfBirth: string (nullable = true)\n",
      " |-- FullName: string (nullable = true)\n",
      " |-- IsCaptain: boolean (nullable = true)\n",
      " |-- Number: string (nullable = true)\n",
      " |-- Position: string (nullable = true)\n",
      " |-- Team: string (nullable = true)\n",
      " |-- Year: long (nullable = true)\n",
      " |-- date2: date (nullable = true)\n",
      " |-- date3: date (nullable = true)\n",
      " |-- NumberInt: integer (nullable = true)\n",
      " |-- date1: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "toDate = udf(lambda x: datetime.strptime(x, '%m/%d/%Y'), DateType())\n",
    "\n",
    "wcDF = wcDF.withColumn('date1', toDate(wcDF['DateOfBirth']))\n",
    "wcDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date\n",
    "\n",
    "wcDF=wcDF.withColumn('date2', to_date(wcDF['DateOfBirth']))\n",
    "\n",
    "#to_date는 위의 datetime보다 쉬운 방법이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Club: string (nullable = true)\n",
      " |-- ClubCountry: string (nullable = true)\n",
      " |-- Competition: string (nullable = true)\n",
      " |-- DateOfBirth: string (nullable = true)\n",
      " |-- FullName: string (nullable = true)\n",
      " |-- IsCaptain: boolean (nullable = true)\n",
      " |-- Number: string (nullable = true)\n",
      " |-- Position: string (nullable = true)\n",
      " |-- Team: string (nullable = true)\n",
      " |-- Year: long (nullable = true)\n",
      " |-- date2: date (nullable = true)\n",
      " |-- date3: date (nullable = true)\n",
      " |-- NumberInt: integer (nullable = true)\n",
      " |-- date1: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import DateType\n",
    "\n",
    "wcDF=wcDF.withColumn('date3', wcDF['DateOfBirth'].cast(DateType()))\n",
    "wcDF=wcDF.withColumn('NumberInt', wcDF['Number'].cast(\"integer\"))\n",
    "\n",
    "wcDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "wcDF=wcDF.drop('date1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Club=u'Club Atl\\xc3\\xa9tico Talleres de Remedios de Escalada', ClubCountry=u'Argentina', Competition=u'World Cup', DateOfBirth=u'1905-5-5', FullName=u'\\xc3ngel Bossio', IsCaptain=False, Number=u'', Position=u'GK', Team=u'Argentina', Year=1930, date2=datetime.date(1905, 5, 5), date3=datetime.date(1905, 5, 5), NumberInt=None)]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wcDF.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|ClubCountry|count|\n",
      "+-----------+-----+\n",
      "|   England |    4|\n",
      "|   Paraguay|   93|\n",
      "|     Russia|   51|\n",
      "|        POL|   11|\n",
      "|        BRA|   27|\n",
      "|    Senegal|    1|\n",
      "|     Sweden|  154|\n",
      "|   Colombia|    1|\n",
      "|        FRA|  155|\n",
      "|        ALG|    8|\n",
      "|   England |    1|\n",
      "|       RUS |    1|\n",
      "|     Turkey|   65|\n",
      "|      Zaire|   22|\n",
      "|       Iraq|   22|\n",
      "|    Germany|  206|\n",
      "|        RSA|   16|\n",
      "|        ITA|  224|\n",
      "|        UKR|   38|\n",
      "|        GHA|    8|\n",
      "+-----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wcDF.groupBy(wcDF.ClubCountry).count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----+----+----+----+----+\n",
      "|ClubCountry|    |  DF|  FW|  GK|  MF|\n",
      "+-----------+----+----+----+----+----+\n",
      "|   England |null|null|   2|null|   2|\n",
      "|   Paraguay|null|  26|  37|  10|  20|\n",
      "|     Russia|null|  20|  11|   4|  16|\n",
      "|        POL|null|   2|   2|   3|   4|\n",
      "|        BRA|null|   7|   5|   4|  11|\n",
      "|    Senegal|null|null|null|   1|null|\n",
      "|     Sweden|null|  40|  47|  25|  42|\n",
      "|   Colombia|null|null|   1|null|null|\n",
      "|        ALG|null|   2|null|   6|null|\n",
      "|        FRA|null|  46|  41|  18|  50|\n",
      "|   England |null|null|null|null|   1|\n",
      "|       RUS |null|null|null|   1|null|\n",
      "|     Turkey|null|  20|  13|  12|  20|\n",
      "|      Zaire|null|   6|   5|   3|   8|\n",
      "|       Iraq|null|   6|   4|   3|   9|\n",
      "|    Germany|null|  64|  51|  16|  75|\n",
      "|        RSA|null|   5|   2|   3|   6|\n",
      "|        UKR|null|  13|   7|   4|  14|\n",
      "|        ITA|null|  74|  42|  19|  89|\n",
      "|        CMR|null|   1|   1|   1|null|\n",
      "+-----------+----+----+----+----+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wcDF.groupBy('ClubCountry').pivot('Position').count().show()\n",
    "#pivot() 험수 안의 인자는 컬럼 명이 들어간다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- year: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- height: integer (nullable = true)\n",
      "\n",
      "+----+-------+------+\n",
      "|year|   name|height|\n",
      "+----+-------+------+\n",
      "|   1|kim, js|   170|\n",
      "|   1|lee, sm|   175|\n",
      "|   2|lim, yg|   180|\n",
      "|   2|    lee|   170|\n",
      "+----+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf = spark.read.format('com.databricks.spark.csv')\\\n",
    "    .options(header='true', inferschema='true').load('data/myDf.csv')\n",
    "\n",
    "myDf.printSchema()\n",
    "myDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Club: string (nullable = true)\n",
      " |-- ClubNation: string (nullable = true)\n",
      " |-- Competition: string (nullable = true)\n",
      " |-- DateOfBirth: string (nullable = true)\n",
      " |-- FullName: string (nullable = true)\n",
      " |-- IsCaptain: boolean (nullable = true)\n",
      " |-- Number: string (nullable = true)\n",
      " |-- Position: string (nullable = true)\n",
      " |-- Team: string (nullable = true)\n",
      " |-- Year: long (nullable = true)\n",
      " |-- date2: date (nullable = true)\n",
      " |-- date3: date (nullable = true)\n",
      " |-- NumberInt: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wcDF=wcDF.withColumnRenamed('ClubCountry','ClubNation') # 컬럼 네임 변경\n",
    "wcDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<name>"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myDf.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<name>"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myDf['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(name=u'kim, js'),\n",
       " Row(name=u'lee, sm'),\n",
       " Row(name=u'lim, yg'),\n",
       " Row(name=u'lee')]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_name=myDf.select('name')\n",
    "_name.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'kim', u' js']\n"
     ]
    }
   ],
   "source": [
    "r=Row(name=u'kim, js')\n",
    "rd=r.asDict()\n",
    "print rd.values()[0].split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+\n",
      "|   name|height|\n",
      "+-------+------+\n",
      "|kim, js|   170|\n",
      "|    lee|   170|\n",
      "+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.where(myDf['height'] < 175)\\\n",
    "    .select(myDf['name'], myDf['height']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+-----------+\n",
      "|year|max(year)|max(height)|\n",
      "+----+---------+-----------+\n",
      "|   1|        1|        175|\n",
      "|   2|        2|        180|\n",
      "+----+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.groupby(myDf['year']).max().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- year: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- height: integer (nullable = true)\n",
      " |-- heightD: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "#udf는 사용자 정의 함수. 함수를 만들어 쓸 수 있다.\n",
    "toDoublefunc = udf(lambda x: float(x),DoubleType())\n",
    "myDf = myDf.withColumn(\"heightD\",toDoublefunc(myDf.height))\n",
    "\n",
    "myDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf, struct\n",
    "from pyspark.sql.types import IntegerType\n",
    "toint=udf(lambda x:int(x),IntegerType())\n",
    "myDf=myDf.withColumn(\"yearI\",toint(myDf['year']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- year: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- height: integer (nullable = true)\n",
      " |-- heightD: double (nullable = true)\n",
      " |-- yearI: integer (nullable = true)\n",
      "\n",
      "+----+-------+------+-------+-----+\n",
      "|year|   name|height|heightD|yearI|\n",
      "+----+-------+------+-------+-----+\n",
      "|   1|kim, js|   170|  170.0|    1|\n",
      "|   1|lee, sm|   175|  175.0|    1|\n",
      "|   2|lim, yg|   180|  180.0|    2|\n",
      "|   2|    lee|   170|  170.0|    2|\n",
      "+----+-------+------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.printSchema()\n",
    "myDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|yearI|\n",
      "+-----+\n",
      "|    1|\n",
      "|    1|\n",
      "|    2|\n",
      "|    2|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.select('yearI').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+------+-------+-----+---------+\n",
      "|year|   name|height|heightD|yearI|nameUpper|\n",
      "+----+-------+------+-------+-----+---------+\n",
      "|   1|kim, js|   170|  170.0|    1|  KIM, JS|\n",
      "|   1|lee, sm|   175|  175.0|    1|  LEE, SM|\n",
      "|   2|lim, yg|   180|  180.0|    2|  LIM, YG|\n",
      "|   2|    lee|   170|  170.0|    2|      LEE|\n",
      "+----+-------+------+-------+-----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import udf\n",
    " \n",
    "def uppercase(s):\n",
    "    return s.upper()\n",
    "\n",
    "upperUdf = udf(uppercase, StringType())\n",
    "myDf = myDf.withColumn(\"nameUpper\", upperUdf(myDf['name']))\n",
    "myDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+------+-------+-----+---------+----------+\n",
      "|year|   name|height|heightD|yearI|nameUpper|height>175|\n",
      "+----+-------+------+-------+-----+---------+----------+\n",
      "|   1|kim, js|   170|  170.0|    1|  KIM, JS|   shorter|\n",
      "|   1|lee, sm|   175|  175.0|    1|  LEE, SM|    taller|\n",
      "|   2|lim, yg|   180|  180.0|    2|  LIM, YG|    taller|\n",
      "|   2|    lee|   170|  170.0|    2|      LEE|   shorter|\n",
      "+----+-------+------+-------+-----+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import udf\n",
    "#키를 Boolean(이진법)으로 나누어보자\n",
    "height_udf = udf(lambda height: \"taller\" if height >=175 else \"shorter\", StringType())\n",
    "heightDf=myDf.withColumn(\"height>175\", height_udf(myDf.heightD))\n",
    "heightDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'shorter'"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "height = 100\n",
    "\"taller\" if height >=175 else \"shorter\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+------------------+------------------+\n",
      "|summary|              year|            height|           heightD|             yearI|\n",
      "+-------+------------------+------------------+------------------+------------------+\n",
      "|  count|                 4|                 4|                 4|                 4|\n",
      "|   mean|               1.5|            173.75|            173.75|               1.5|\n",
      "| stddev|0.5773502691896257|4.7871355387816905|4.7871355387816905|0.5773502691896257|\n",
      "|    min|                 1|               170|             170.0|                 1|\n",
      "|    max|                 2|               180|             180.0|                 2|\n",
      "+-------+------------------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.describe().show() #describe를 쓰면 요약 값을 볼 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#SPARK SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Club: string (nullable = true)\n",
      " |-- ClubNation: string (nullable = true)\n",
      " |-- Competition: string (nullable = true)\n",
      " |-- DateOfBirth: string (nullable = true)\n",
      " |-- FullName: string (nullable = true)\n",
      " |-- IsCaptain: boolean (nullable = true)\n",
      " |-- Number: string (nullable = true)\n",
      " |-- Position: string (nullable = true)\n",
      " |-- Team: string (nullable = true)\n",
      " |-- Year: long (nullable = true)\n",
      " |-- date2: date (nullable = true)\n",
      " |-- date3: date (nullable = true)\n",
      " |-- NumberInt: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wcDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+----+\n",
      "|                Club|     Team|Year|\n",
      "+--------------------+---------+----+\n",
      "|Club AtlÃ©tico Ta...|Argentina|1930|\n",
      "+--------------------+---------+----+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wcDF.createOrReplaceTempView(\"wc\") # 테이블이 있으면 불러오고 없으면 보여줘\n",
    "spark.sql(\"select Club,Team,Year from wc\").show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+---------+----+\n",
      "|    FullName|                Club|     Team|Year|\n",
      "+------------+--------------------+---------+----+\n",
      "|Ãngel Bossio|Club AtlÃ©tico Ta...|Argentina|1930|\n",
      "+------------+--------------------+---------+----+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wcPlayers=spark.sql(\"select FullName,Club,Team,Year from wc\")\n",
    "wcPlayers.show(1) #SQL 쓸줄 알면 이렇게 쓰는게 좋을듯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name=u'wc', database=None, description=None, tableType=u'TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full name: Ãngel Bossio\n",
      "Full name: Juan Botasso\n",
      "Full name: Roberto Cherro\n",
      "Full name: Alberto Chividini\n",
      "Full name: \n"
     ]
    }
   ],
   "source": [
    "namesRdd=wcPlayers.rdd.map(lambda x: \"Full name: \"+x[0]) #x[0] : name\n",
    "for e in namesRdd.take(5):\n",
    "    print e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------------------------+\n",
      "|bucketId|items                       |\n",
      "+--------+----------------------------+\n",
      "|1       |[orange, apple, pineapple]  |\n",
      "|2       |[watermelon, apple, bananas]|\n",
      "+--------+----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#sql functions\n",
    "\n",
    "bucketDf=spark.createDataFrame([[1,[\"orange\", \"apple\", \"pineapple\"]],\n",
    "                                [2,[\"watermelon\",\"apple\",\"bananas\"]]],\n",
    "                               [\"bucketId\",\"items\"])\n",
    "\n",
    "bucketDf.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+\n",
      "|bucketId|      item|\n",
      "+--------+----------+\n",
      "|       1|    orange|\n",
      "|       1|     apple|\n",
      "|       1| pineapple|\n",
      "|       2|watermelon|\n",
      "|       2|     apple|\n",
      "|       2|   bananas|\n",
      "+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "bDf=bucketDf.select(bucketDf.bucketId,explode(bucketDf.items).alias('item'))\n",
    "#item 내부를 쪼갠다\n",
    "bDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+\n",
      "|      item|itemId|\n",
      "+----------+------+\n",
      "|    orange|    F1|\n",
      "|          |    F2|\n",
      "| pineapple|    F3|\n",
      "|watermelon|    F4|\n",
      "|   bananas|    F5|\n",
      "+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fDf=spark.createDataFrame([[\"orange\", \"F1\"],\n",
    "                            [\"\", \"F2\"],\n",
    "                            [\"pineapple\",\"F3\"],\n",
    "                            [\"watermelon\",\"F4\"],\n",
    "                            [\"bananas\",\"F5\"]],\n",
    "                            [\"item\",\"itemId\"])\n",
    "\n",
    "fDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+--------+\n",
      "|itemId|      item|bucketId|\n",
      "+------+----------+--------+\n",
      "|    F5|   bananas|       2|\n",
      "|    F1|    orange|       1|\n",
      "|    F3| pineapple|       1|\n",
      "|    F4|watermelon|       2|\n",
      "+------+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joinDf=fDf.join(bDf, fDf.item==bDf.item, \"inner\")\n",
    "#join은 테이블을 연결한다.\n",
    "#join을 쓰면 기준에 일치하지 않는 데이터를 제외한다.\n",
    "joinDf.select(fDf.itemId,fDf.item,bDf.bucketId).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+------+-------+-----+---------+\n",
      "|year|   name|height|heightD|yearI|nameUpper|\n",
      "+----+-------+------+-------+-----+---------+\n",
      "|   1|kim, js|   170|  170.0|    1|  KIM, JS|\n",
      "|   1|lee, sm|   175|  175.0|    1|  LEE, SM|\n",
      "|   2|lim, yg|   180|  180.0|    2|  LIM, YG|\n",
      "|   2|    lee|   170|  170.0|    2|      LEE|\n",
      "+----+-------+------+-------+-----+---------+\n",
      "\n",
      "+--------+\n",
      "|how tall|\n",
      "+--------+\n",
      "|    <175|\n",
      "|    <175|\n",
      "|    >175|\n",
      "|    <175|\n",
      "+--------+\n",
      "\n",
      "+----+------------+\n",
      "|year|avg(heightD)|\n",
      "+----+------------+\n",
      "|   1|       172.5|\n",
      "|   2|       175.0|\n",
      "+----+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.show()\n",
    "\n",
    "from pyspark.sql.functions import when\n",
    "_myDf=myDf.select(when(myDf['heightD'] >175.0, \">175\")\\\n",
    "            .otherwise(\"<175\").alias(\"how tall\"))\n",
    "_myDf.show()\n",
    "\n",
    "myDf.groupBy('year').agg({\"heightD\":\"avg\"}).show()\n",
    "#agg ->합계 함수(avg, sum, max, min, count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+------------+------------+\n",
      "|min(heightD)|max(heightD)|avg(heightD)|sum(heightD)|\n",
      "+------------+------------+------------+------------+\n",
      "|       170.0|       180.0|      173.75|       695.0|\n",
      "+------------+------------+------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "myDf.agg(F.min(myDf.heightD),F.max(myDf.heightD),F.avg(myDf.heightD),F.sum(myDf.heightD)).show()\n",
    "#sql 함수 별도 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#과제 네트워크 침입 분석\n",
    "#데이터 입력 -> attack/un-attack 분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib\n",
    "_rdd = spark.sparkContext\\\n",
    "    .textFile(os.path.join('data', 'kddcup.data.gz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'0,tcp,http,SF,215,45076,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,1,0.00,0.00,0.00,0.00,1.00,0.00,0.00,0,0,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,normal.',\n",
       " u'0,tcp,http,SF,162,4528,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,2,2,0.00,0.00,0.00,0.00,1.00,0.00,0.00,1,1,1.00,0.00,1.00,0.00,0.00,0.00,0.00,0.00,normal.',\n",
       " u'0,tcp,http,SF,236,1228,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,1,0.00,0.00,0.00,0.00,1.00,0.00,0.00,2,2,1.00,0.00,0.50,0.00,0.00,0.00,0.00,0.00,normal.',\n",
       " u'0,tcp,http,SF,233,2032,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,2,2,0.00,0.00,0.00,0.00,1.00,0.00,0.00,3,3,1.00,0.00,0.33,0.00,0.00,0.00,0.00,0.00,normal.',\n",
       " u'0,tcp,http,SF,239,486,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,3,3,0.00,0.00,0.00,0.00,1.00,0.00,0.00,4,4,1.00,0.00,0.25,0.00,0.00,0.00,0.00,0.00,normal.',\n",
       " u'0,tcp,http,SF,238,1282,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,4,4,0.00,0.00,0.00,0.00,1.00,0.00,0.00,5,5,1.00,0.00,0.20,0.00,0.00,0.00,0.00,0.00,normal.',\n",
       " u'0,tcp,http,SF,235,1337,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,5,5,0.00,0.00,0.00,0.00,1.00,0.00,0.00,6,6,1.00,0.00,0.17,0.00,0.00,0.00,0.00,0.00,normal.',\n",
       " u'0,tcp,http,SF,234,1364,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,6,6,0.00,0.00,0.00,0.00,1.00,0.00,0.00,7,7,1.00,0.00,0.14,0.00,0.00,0.00,0.00,0.00,normal.',\n",
       " u'0,tcp,http,SF,239,1295,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,7,7,0.00,0.00,0.00,0.00,1.00,0.00,0.00,8,8,1.00,0.00,0.12,0.00,0.00,0.00,0.00,0.00,normal.',\n",
       " u'0,tcp,http,SF,181,5450,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,8,8,0.00,0.00,0.00,0.00,1.00,0.00,0.00,9,9,1.00,0.00,0.11,0.00,0.00,0.00,0.00,0.00,normal.']"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_rdd.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4898431"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[u'0', u'tcp', u'http', u'SF', u'215', u'45076', u'0', u'0', u'0', u'0', u'0', u'1', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'1', u'1', u'0.00', u'0.00', u'0.00', u'0.00', u'1.00', u'0.00', u'0.00', u'0', u'0', u'0.00', u'0.00', u'0.00', u'0.00', u'0.00', u'0.00', u'0.00', u'0.00', u'normal.']]\n"
     ]
    }
   ],
   "source": [
    "_allRdd=_rdd.map(lambda x:x.split(','))\n",
    "print _allRdd.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'guess_passwd.', 53),\n",
       " (u'nmap.', 2316),\n",
       " (u'warezmaster.', 20),\n",
       " (u'rootkit.', 10),\n",
       " (u'warezclient.', 1020),\n",
       " (u'smurf.', 2807886),\n",
       " (u'pod.', 264),\n",
       " (u'neptune.', 1072017),\n",
       " (u'normal.', 972781),\n",
       " (u'spy.', 2),\n",
       " (u'ftp_write.', 8),\n",
       " (u'phf.', 4),\n",
       " (u'portsweep.', 10413),\n",
       " (u'teardrop.', 979),\n",
       " (u'buffer_overflow.', 30),\n",
       " (u'land.', 21),\n",
       " (u'imap.', 12),\n",
       " (u'loadmodule.', 9),\n",
       " (u'perl.', 3),\n",
       " (u'multihop.', 7),\n",
       " (u'back.', 2203),\n",
       " (u'ipsweep.', 12481),\n",
       " (u'satan.', 15892)]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " #41번째에 침입의 종류 나와있다 -> 침입의 종류를 키로 하고 Value를 1로(count용)\n",
    "_fO.reduceByKey(lambda x, y : x+y).collect() #reduceByKey를 써서 Key별로 바로 count 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "972781\n",
      "3925650\n"
     ]
    }
   ],
   "source": [
    "_normal=_allRdd.filter(lambda x:x[41]==\"normal.\")\n",
    "_attack=_allRdd.filter(lambda x:x[41]!=\"normal.\")\n",
    "\n",
    "print _normal.count()\n",
    "print _attack.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- attack: string (nullable = true)\n",
      " |-- dst_bytes: long (nullable = true)\n",
      " |-- duration: long (nullable = true)\n",
      " |-- flag: string (nullable = true)\n",
      " |-- protocol: string (nullable = true)\n",
      " |-- service: string (nullable = true)\n",
      " |-- src_bytes: long (nullable = true)\n",
      "\n",
      "+-------+---------+--------+----+--------+-------+---------+\n",
      "| attack|dst_bytes|duration|flag|protocol|service|src_bytes|\n",
      "+-------+---------+--------+----+--------+-------+---------+\n",
      "|normal.|    45076|       0|  SF|     tcp|   http|      215|\n",
      "|normal.|     4528|       0|  SF|     tcp|   http|      162|\n",
      "|normal.|     1228|       0|  SF|     tcp|   http|      236|\n",
      "|normal.|     2032|       0|  SF|     tcp|   http|      233|\n",
      "|normal.|      486|       0|  SF|     tcp|   http|      239|\n",
      "+-------+---------+--------+----+--------+-------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "_csv = _rdd.map(lambda l:l.split(\",\"))\n",
    "_csvRdd = _csv.map(lambda p:\n",
    "    Row(\n",
    "        duration = int(p[0]),\n",
    "        protocol = p[1],\n",
    "        service = p[2],\n",
    "        flag = p[3],\n",
    "        src_bytes=int(p[4]),\n",
    "        dst_bytes=int(p[5]),\n",
    "        attack=p[41]\n",
    "    )\n",
    ")\n",
    "\n",
    "_df =spark.createDataFrame(_csvRdd)\n",
    "_df.printSchema()\n",
    "_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "attack_udf=udf(lambda x: \"normal\" if x ==\"normal.\" else \"attack\", StringType())\n",
    "myAdf=_df.withColumn(\"attackB\", attack_udf(_df.attack))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- year: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- height: integer (nullable = true)\n",
      " |-- heightD: double (nullable = true)\n",
      " |-- yearI: integer (nullable = true)\n",
      " |-- nameUpper: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
